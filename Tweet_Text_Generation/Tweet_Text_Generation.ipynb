{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC321 Tutorial 11: Text Generation\n",
    "\n",
    "In this tutorial, we will use recurrent neural networks to **generate** \n",
    "sequences. \n",
    "Generating sequences is more involved comparing to making predictions about\n",
    "sequences. However, it is a very interesting task. This tutorial will help\n",
    "prepare you for project 4.\n",
    "\n",
    "Much of today's content is an adaptation of the \"Practical PyTorch\" github \n",
    "repository [1], and Lisa's notes on Generating Trump Tweets [2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1] https://github.com/spro/practical-pytorch/blob/master/char-rnn-generation/char-rnn-generation.ipynb\n",
    "# [2] https://www.cs.toronto.edu/~lczhang/360/lec/w08/gen.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reccurent Neural Networks Revisited\n",
    "\n",
    "The recurrent neural network architecture from last time \n",
    "looked something like this:\n",
    "\n",
    "<img src=\"https://www.cs.toronto.edu/~lczhang/321/tut/imgs/rnn.png\" width=\"500px\">\n",
    "\n",
    "The input sequence is broken down into tokens. We could choose whether to\n",
    "tokenize based on words, or based on characters. The representation\n",
    "of each token (GloVe or one-hot) is processed by the RNN one step at a time\n",
    "to update the hidden (or context) state.\n",
    "\n",
    "In a predictive RNN, the value of the\n",
    "hidden states  is a representation of **all the text that was processed thus far**.\n",
    "Similarly, in a generative RNN, The value of the hidden\n",
    "state will be a representation of **all the text that still needs to be generated**.\n",
    "We will use this hidden state to produce the sequence, one token at a time.\n",
    "\n",
    "Similar to last class we will break up the problem of generating text\n",
    "to generating one token at a time.\n",
    "\n",
    "We will do so with the help of two functions:\n",
    "\n",
    "1. We need to be able to generate the *next* token, given the current \n",
    "   hidden state. In practice, we get a probability distribution over \n",
    "   the next token, and sample from that probability distribution.\n",
    "2. We need to be able to update the hidden state somehow. To do so,\n",
    "   we need two piece of information: the old hidden state, and the actual\n",
    "   token that was generated in the previous step. The actual token generated\n",
    "   will inform the subsequent tokens.\n",
    "\n",
    "We will repeat both functions until a special \"END OF SEQUENCE\" token is\n",
    "generated. Here is a pictorial representation of what we will do:\n",
    "\n",
    "<img src=\"https://www.cs.toronto.edu/~lczhang/321/tut/imgs/rnn_gen_figure.png\" width=\"500px\">\n",
    "\n",
    "Note that there are several tricky things that we will have to figure out.\n",
    "For example, how do we actually sample the actual token from the probability\n",
    "distribution over tokens? What would we do during training, and how might \n",
    "that be different from during testing/evaluation? We will answer those\n",
    "questions as we implement the RNN.\n",
    "\n",
    "For now, let's start with our training data.\n",
    "\n",
    "## Data: Happy Tweets from 2018\n",
    "\n",
    "The training set we use is a collection of \"happy\" tweets from the Sentiment140\n",
    "data set.\n",
    "We will only use tweets that are 140 characters or shorter, and tweets\n",
    "that contains more than just a URL.\n",
    "Since tweets often contain creative spelling and numbers, and upper vs lower\n",
    "case characters are read very differently, we will use a character-level RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "tweets = list(line[0] for line in csv.reader(open('happy.csv', encoding=\"utf8\")))\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a few of these tweets, just to get a sense of the kind of text\n",
    "we're dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@_hayles yeh!!!nowt wrong with my wee fingers  type type type.. lol oohh spray tan!never had 1 b4\n",
      "@graemerawson Thanks. Probably looking for someone local\n",
      "@michaelahills YEP \n"
     ]
    }
   ],
   "source": [
    "print(tweets[100])\n",
    "print(tweets[1000])\n",
    "print(tweets[10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating One Tweet\n",
    "\n",
    "Our generative RNN is a **generative model**. At a high level, generative\n",
    "models are trained to maximize the probability of generating our training data.\n",
    "We'll need to figure out what that means, and to manipulate our data \n",
    "Our RNN model generates text one character at a time. In each iteration,\n",
    "we'll train our data to generate a different tweet.\n",
    "\n",
    "Normally, when we build a new machine learn model, we want to make sure\n",
    "that our model can overfit. To that end, we will first build a neural network\n",
    "that can generate _one_ tweet really well. We can choose any tweet (or any other text)\n",
    "we want.  Let's choose to build an RNN that generates `tweet[200]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@whin5 nice one '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = tweets[200]\n",
    "tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since tweets are often poorly spelled, we will treat a tweet as a **sequence of characters**.\n",
    "Our RNN will generate one **character** as a time.\n",
    "\n",
    "Since PyTorch works with numbers instead of strings, we will need to convert\n",
    "characters into integers. This of this integer as a sparse representation of a\n",
    "one-hot encoding.  We'll build dictionary mappings\n",
    "from the character to the index of that character (a unique integer identifier),\n",
    "and from the index to the character. We'll use the same naming scheme that `torchtext`\n",
    "uses (`stoi` and `itos`, which means \"string to index\" and \"index to string\").\n",
    "\n",
    "For simplicity, we'll work with a limited vocabulary containing\n",
    "just the characters in `tweet[200]`, plus two special tokens:\n",
    "\n",
    "- `<EOS>` represents \"End of String\", which we'll append to the end of our tweet.\n",
    "  Since tweets are variable-length, this is a way for the RNN to signal\n",
    "  that the entire sequence has been generated.\n",
    "- `<BOS>` represents \"Beginning of String\", which we'll prepend to the beginning of \n",
    "  our tweet. This is the first token that we will feed into the RNN.\n",
    "\n",
    "The way we use these special tokens will become more clear as we build the model.\n",
    "\n",
    "### Part (a)\n",
    "\n",
    "Build two Python dictionary mappings\n",
    "from the character to the index of that character (a unique integer identifier),\n",
    "and from the index to the character. We'll use the same naming scheme that `torchtext`\n",
    "uses (`stoi` and `itos`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(tweet)) + [\"<BOS>\", \"<EOS>\"]\n",
    "vocab_stoi = {} # TODO: build a dictionary mapping of word to unique index\n",
    "vocab_stoi= {s: i for i, s in enumerate(vocab)}\n",
    "\n",
    "vocab_itos = {} # TODO: build a dictionary mapping of a unique index to a word (string)\n",
    "vocab_itos = {i: s for i, s in enumerate(vocab)}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_stoi['<BOS>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b)\n",
    "\n",
    "Our model have three parts:\n",
    "\n",
    "1. An **embedding** layer that takes a character index (sparse representation of a one-hot vector\n",
    "   representing the character), and returns an **embedding** ${\\bf x}^{(t)}$ for the character.\n",
    "   We could use a one-hot embedding here, but using a low-dimensional but dense embedding\n",
    "   is more powerful, and is more in line with what we will do in Project 4.\n",
    "2. A **recurrent neural network** layer. We will use a `nn.GRU` unit. At each time step,\n",
    "   this layer will take the previous hidden state ${\\bf h}^{(t-1)}$ and the embedding of\n",
    "   a new generated token ${\\bf x}^{(t)}$ and compute the new hidden state ${\\bf h}^{(t)}$ representing\n",
    "   *tokens not yet generated*.\n",
    "3. The **projection MLP** (a fully-connected layer) that takes the hidden state ${\\bf h}^{(t)}$\n",
    "   and computes teh distribution of the next character to generate.\n",
    "\n",
    "Filling in the missing numbers in the `__init__` method using\n",
    "the parameters `vocab_size`, `emb_size`, and `hidden_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,     # number of unique characters in our vocabulary\n",
    "                 embedding_size, # size of the word embeddings ${\\bf x}^{(t)}$\n",
    "                 hidden_size):   # size of the hidden state in the RNN\n",
    "        super(TextGenerator, self).__init__()\n",
    "\n",
    "        # Embedding \n",
    "        self.embed = nn.Embedding(num_embeddings=vocab_size, # TODO\n",
    "                                  embedding_dim=embedding_size)  # TODO\n",
    "\n",
    "        # recurrent neural network\n",
    "        self.rnn = nn.GRU(input_size=embedding_size, #TODO\n",
    "                          hidden_size=hidden_size, #TODO\n",
    "                          batch_first=True)\n",
    "\n",
    "        # a fully-connect layer that outputs a distribution over\n",
    "        # the next token, given the RNN output\n",
    "        self.proj = nn.Linear(in_features=hidden_size, # TODO\n",
    "                              out_features=vocab_size) # TODO\n",
    "\n",
    "    def forward(self, inp, hidden=None):\n",
    "        emb = self.embed(inp)                  # generate one-hot vectors of input\n",
    "        output, hidden = self.rnn(emb, hidden) # get the next output and hidden state\n",
    "        output = self.proj(output)             # predict distribution over next tokens\n",
    "        return output, hidden\n",
    "\n",
    "model = TextGenerator(vocab_size, 128, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Teacher Forcing\n",
    "\n",
    "At a very high level, we want our RNN model to have a high probability\n",
    "of generating the tweet. An RNN model generates text\n",
    "one character at a time based on the hidden state value.\n",
    "At each time step, we will check whether the mdoel generated the\n",
    "correct character. That is, at each time step,\n",
    "we are trying to select the correct next character out of all the \n",
    "characters in our vocabulary. Recall that this problem is a multi-class\n",
    "classification problem, and we can use Cross-Entropy loss to train our\n",
    "network to become better at this type of problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we don't just have a single multi-class classification problem.\n",
    "Instead, we have **one classification problem per time-step** (per token)!\n",
    "So, how do we predict the first token in the sequence? \n",
    "How do we predict the second token in the sequence? \n",
    "\n",
    "To help you understand what happens durign RNN training, we'll start with a\n",
    "inefficient training code that shows you what happens step-by-step. We'll\n",
    "start with computing the loss for the first token generated, then the second token,\n",
    "and so on.\n",
    "Later on, we'll switch to a simpler and more performant version of the code.\n",
    "\n",
    "So, let's start with the first classification problem: the problem of generating\n",
    "the **first** token (`tweet[0]`).\n",
    "\n",
    "<img src=\"https://www.cs.toronto.edu/~lczhang/321/tut/imgs/rnn_gen_figure.png\" width=\"500px\">\n",
    "\n",
    "To generate the first token, we'll feed the RNN network (with an initial, empty\n",
    "hidden state) the \"<BOS>\" token. Then, the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0022,  0.4379,  0.0921,  0.0174,  0.1430, -0.0266, -0.0144,\n",
       "          -0.0998, -0.1854,  0.1171,  0.0660, -0.2929]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bos_input = torch.Tensor([vocab_stoi[\"<BOS>\"]]).long().unsqueeze(0)\n",
    "output, hidden = model(bos_input, hidden=None)\n",
    "output # distribution over the first token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the loss using `criterion`. Since the model is untrained,\n",
    "the loss is expected to be high. (For now, we won't do anything\n",
    "with this loss, and omit the backward pass.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5042, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.Tensor([vocab_stoi[tweet[0]]]).long().unsqueeze(0)\n",
    "criterion(output.reshape(-1, vocab_size), # reshape to 2D tensor\n",
    "          target.reshape(-1))             # reshape to 1D tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to update the hidden state and generate a prediction\n",
    "for the next token. To do so, we need to provide the current token to\n",
    "the RNN. We already said that during test time, we'll need to sample\n",
    "from the predicted probabilty over tokens that the neural network\n",
    "just generated. \n",
    "\n",
    "Right now, we can do something better: we can **use the ground-truth,\n",
    "actual target token**. This technique is called **teacher-forcing**, \n",
    "and generally speeds up training. The reason is that right now, \n",
    "since our model does not perform well, the predicted probability\n",
    "distribution is pretty far from the ground truth. So, it is very,\n",
    "very difficult for the neural network to get back on track given bad\n",
    "input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1702,  0.1959, -0.0536, -0.2001,  0.0015, -0.0085, -0.0152,\n",
       "          -0.1355, -0.1310,  0.1309, -0.1383, -0.1810]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use teacher-forcing: we pass in the ground truth `target`,\n",
    "# rather than using the NN predicted distribution\n",
    "output, hidden = model(target, hidden)\n",
    "output # distribution over the second token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the first step, we can compute the loss, quantifying the\n",
    "difference between the predicted distribution and the actual next\n",
    "token. This loss can be used to adjust the weights of the neural\n",
    "network (which we are not doing yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4716, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.Tensor([vocab_stoi[tweet[1]]]).long().unsqueeze(0)\n",
    "criterion(output.reshape(-1, vocab_size), # reshape to 2D tensor\n",
    "          target.reshape(-1))             # reshape to 1D tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can continue this process of:\n",
    "\n",
    "- feeding the previous ground-truth token to the RNN,\n",
    "- obtaining the prediction distribution over the next token, and\n",
    "- computing the loss,\n",
    "\n",
    "for as many steps as there are tokens in the ground-truth tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 tensor([[[ 0.2488,  0.2693,  0.0383,  0.1714,  0.0561, -0.0518, -0.0662,\n",
      "          -0.0547, -0.2378, -0.1083, -0.0794, -0.1511]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.5514, grad_fn=<NllLossBackward>)\n",
      "3 tensor([[[ 0.2772,  0.2910,  0.1358,  0.3478,  0.0824, -0.0323, -0.1525,\n",
      "          -0.0234, -0.2543, -0.2274, -0.0347, -0.1526]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.5584, grad_fn=<NllLossBackward>)\n",
      "4 tensor([[[ 0.2848,  0.2975,  0.2124,  0.4299,  0.0926, -0.0137, -0.2117,\n",
      "          -0.0090, -0.2483, -0.2883, -0.0066, -0.1618]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.5559, grad_fn=<NllLossBackward>)\n",
      "5 tensor([[[ 0.2842,  0.2996,  0.2646,  0.4682,  0.0951,  0.0020, -0.2482,\n",
      "          -0.0028, -0.2408, -0.3215,  0.0091, -0.1726]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.5489, grad_fn=<NllLossBackward>)\n",
      "6 tensor([[[ 0.2805,  0.3011,  0.2982,  0.4861,  0.0949,  0.0154, -0.2704,\n",
      "          -0.0012, -0.2360, -0.3403,  0.0169, -0.1832]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.5400, grad_fn=<NllLossBackward>)\n",
      "7 tensor([[[ 0.2760,  0.3029,  0.3195,  0.4941,  0.0938,  0.0265, -0.2839,\n",
      "          -0.0022, -0.2334, -0.3512,  0.0203, -0.1926]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.5310, grad_fn=<NllLossBackward>)\n",
      "8 tensor([[[ 0.2716,  0.3049,  0.3331,  0.4975,  0.0925,  0.0353, -0.2921,\n",
      "          -0.0043, -0.2321, -0.3575,  0.0214, -0.2005]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.5232, grad_fn=<NllLossBackward>)\n",
      "9 tensor([[[ 0.2677,  0.3070,  0.3419,  0.4986,  0.0913,  0.0420, -0.2971,\n",
      "          -0.0066, -0.2313, -0.3611,  0.0213, -0.2070]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.5168, grad_fn=<NllLossBackward>)\n",
      "10 tensor([[[ 0.2643,  0.3090,  0.3479,  0.4987,  0.0901,  0.0471, -0.3002,\n",
      "          -0.0089, -0.2308, -0.3632,  0.0208, -0.2120]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.5118, grad_fn=<NllLossBackward>)\n",
      "11 tensor([[[ 0.2616,  0.3106,  0.3519,  0.4984,  0.0891,  0.0508, -0.3022,\n",
      "          -0.0107, -0.2304, -0.3643,  0.0201, -0.2158]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.5080, grad_fn=<NllLossBackward>)\n",
      "12 tensor([[[ 0.2595,  0.3120,  0.3548,  0.4980,  0.0881,  0.0536, -0.3035,\n",
      "          -0.0123, -0.2299, -0.3649,  0.0193, -0.2187]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.5051, grad_fn=<NllLossBackward>)\n",
      "13 tensor([[[ 0.2577,  0.3132,  0.3569,  0.4976,  0.0873,  0.0556, -0.3044,\n",
      "          -0.0134, -0.2296, -0.3651,  0.0186, -0.2209]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.5030, grad_fn=<NllLossBackward>)\n",
      "14 tensor([[[ 0.2564,  0.3140,  0.3584,  0.4972,  0.0867,  0.0570, -0.3050,\n",
      "          -0.0143, -0.2292, -0.3651,  0.0181, -0.2225]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.5015, grad_fn=<NllLossBackward>)\n",
      "15 tensor([[[ 0.2553,  0.3147,  0.3595,  0.4969,  0.0861,  0.0581, -0.3054,\n",
      "          -0.0150, -0.2289, -0.3651,  0.0176, -0.2237]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.5003, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(2, len(tweet)):\n",
    "    output, hidden = model(target, hidden)\n",
    "    target = torch.Tensor([vocab_stoi[tweet[1]]]).long().unsqueeze(0)\n",
    "    loss = criterion(output.reshape(-1, vocab_size), # reshape to 2D tensor\n",
    "                     target.reshape(-1))             # reshape to 1D tensor\n",
    "    print(i, output, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, with our final token, we should expect to output the \"<EOS>\"\n",
    "token, so that our RNN learns when to stop generating characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 tensor([[[ 0.2545,  0.3152,  0.3603,  0.4967,  0.0856,  0.0589, -0.3057,\n",
      "          -0.0155, -0.2286, -0.3650,  0.0171, -0.2246]]],\n",
      "       grad_fn=<AddBackward0>) tensor(2.7830, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output, hidden = model(target, hidden)\n",
    "target = torch.Tensor([vocab_stoi[\"<EOS>\"]]).long().unsqueeze(0)\n",
    "loss = criterion(output.reshape(-1, vocab_size), # reshape to 2D tensor\n",
    "                 target.reshape(-1))             # reshape to 1D tensor\n",
    "print(i, output, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we don't really need a loop. Recall that in a predictive RNN,\n",
    "the `nn.RNN` module can take an entire sequence as input. We can do the\n",
    "same thing here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@',\n",
       " 'w',\n",
       " 'h',\n",
       " 'i',\n",
       " 'n',\n",
       " '5',\n",
       " ' ',\n",
       " 'n',\n",
       " 'i',\n",
       " 'c',\n",
       " 'e',\n",
       " ' ',\n",
       " 'o',\n",
       " 'n',\n",
       " 'e',\n",
       " ' ']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 18])\n"
     ]
    }
   ],
   "source": [
    "tweet_ch = [\"<BOS>\"] + list(tweet) + [\"<EOS>\"]\n",
    "tweet_indices = [vocab_stoi[ch] for ch in tweet_ch]\n",
    "tweet_tensor = torch.Tensor(tweet_indices).long().unsqueeze(0)\n",
    "\n",
    "print(tweet_tensor.shape)\n",
    "\n",
    "output, hidden = model(tweet_tensor[:,:-1]) # <EOS> is never an input token\n",
    "target = tweet_tensor[:,1:]                 # <BOS> is never a target token\n",
    "loss = criterion(output.reshape(-1, vocab_size), # reshape to 2D tensor\n",
    "                 target.reshape(-1))             # reshape to 1D tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10,  3,  5,  2,  9,  6,  8,  0,  6,  9,  1,  4,  0,  7,  6,  4,  0]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_tensor[:,:-1]\n",
    "# target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the input to our neural network model is the *entire*\n",
    "sequence of input tokens (everything from \"<BOS>\" to the\n",
    "last character of the tweet). The neural network generates a prediction distribution\n",
    "of the next token at each step. We can compare each of these  with the ground-truth\n",
    "`target`.\n",
    "\n",
    "### Part (c)\n",
    "\n",
    "Complete the code for the training loop to generate a single tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@whin5 nice one '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 100] Loss 0.012211\n",
      "[Iter 200] Loss 0.004100\n",
      "[Iter 300] Loss 0.002179\n",
      "[Iter 400] Loss 0.001382\n",
      "[Iter 500] Loss 0.000966\n",
      "[Iter 600] Loss 0.000717\n",
      "[Iter 700] Loss 0.000555\n",
      "[Iter 800] Loss 0.000444\n",
      "[Iter 900] Loss 0.000363\n",
      "[Iter 1000] Loss 0.000302\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "for it in range(1000): # You might need to increase this\n",
    "    optimizer.zero_grad()\n",
    "    output, hidden = model(tweet_tensor[:,:-1]) # TODO\n",
    "    loss = criterion(output.reshape(-1, vocab_size),\n",
    "                 target.reshape(-1))      # TODO\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (it+1) % 100 == 0:\n",
    "        print(\"[Iter %d] Loss %f\" % (it+1, float(loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this code runs, your training code should decrease with training,\n",
    "which is what we expect.\n",
    "\n",
    "## Generating a Token\n",
    "\n",
    "At this point, we want to see whether our model is actually learning\n",
    "something. So, we need to talk about how to\n",
    "actually use the RNN model to generate text. If we can \n",
    "generate text, we can make a qualitative asssessment of how well\n",
    "our RNN is performing.\n",
    "\n",
    "The main difference between training and test-time (generation time)\n",
    "is that we don't have the ground-truth tokens to feed as inputs\n",
    "to the RNN. Instead, we need to actually **sample** a token based\n",
    "on the neural network's prediction distribution.\n",
    "\n",
    "But how can we sample a token from a distribution?\n",
    "\n",
    "On one extreme, we can always take\n",
    "the token with the largest probability (argmax). This has been our\n",
    "go-to technique in other classification tasks. However, this idea\n",
    "will fail here. The reason is that in practice, \n",
    "**we want to be able to generate a variety of different sequences from\n",
    "the same model**. An RNN that can only generate a single new tweet\n",
    "is fairly useless.\n",
    "\n",
    "In short, we want some randomness. We can do so by using the logit\n",
    "outputs from our model to construct a multinomial distribution over\n",
    "the tokens, then and sample a random token from that multinomial distribution.\n",
    "\n",
    "One natural multinomial distribution we can choose is the \n",
    "distribution we get after applying the softmax on the outputs.\n",
    "However, we will do one more thing: we will add a **temperature**\n",
    "parameter to manipulate the softmax outputs. We can set a\n",
    "**higher temperature** to make the probability of each token\n",
    "**more even** (more random), or a **lower temperature** to assighn\n",
    "more probability to the tokens with a higher logit (output).\n",
    "A **higher temperature** means that we will get a more diverse sample,\n",
    "with potentially more mistakes. A **lower temperature** means that we\n",
    "may see repetitions of the same high probability sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sequence(model, max_len=100, temperature=0.8):\n",
    "    generated_sequence = \"\"\n",
    "   \n",
    "    inp = torch.Tensor([vocab_stoi[\"<BOS>\"]]).long()\n",
    "    hidden = None\n",
    "    for p in range(max_len):\n",
    "        output, hidden = model(inp.unsqueeze(0), hidden)\n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = int(torch.multinomial(output_dist, 1)[0])\n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = vocab_itos[top_i]\n",
    "        \n",
    "        if predicted_char == \"<EOS>\":\n",
    "            break\n",
    "        generated_sequence += predicted_char       \n",
    "        inp = torch.Tensor([top_i]).long()\n",
    "    return generated_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"input is n\"\n",
    "def sample_sequence2(model, max_len=100, temperature=0.8):\n",
    "    generated_sequence = \"\"\n",
    "   \n",
    "    inp = torch.Tensor([vocab_stoi[\"n\"]]).long()\n",
    "    hidden = None\n",
    "    for p in range(max_len):\n",
    "        output, hidden = model(inp.unsqueeze(0), hidden)\n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = int(torch.multinomial(output_dist, 1)[0])\n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = vocab_itos[top_i]\n",
    "        \n",
    "        if predicted_char == \"<EOS>\":\n",
    "            break\n",
    "        generated_sequence += predicted_char       \n",
    "        inp = torch.Tensor([top_i]).long()\n",
    "    return generated_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 : e \n",
      "1.5 : e \n",
      "0.5 : 5 nice one \n",
      "0.25 : 5 nice one \n",
      "2 : e \n",
      "3 : 5@whin5@\n",
      "4 :  cne @\n",
      "5 : @cw\n"
     ]
    }
   ],
   "source": [
    "'''input to trained model is: n '''\n",
    "print('1.0 :',sample_sequence2(model, temperature=1.0))\n",
    "print('1.5 :',sample_sequence2(model, temperature=1.5))\n",
    "print('0.5 :',sample_sequence2(model, temperature=0.5))\n",
    "print('0.25 :',sample_sequence2(model, temperature=0.25))\n",
    "print('2 :', sample_sequence2(model, temperature=2.0))\n",
    "print('3 :',sample_sequence2(model, temperature=3.0))\n",
    "print('4 :',sample_sequence2(model, temperature=4.0))\n",
    "print('5 :',sample_sequence2(model, temperature=5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (d)\n",
    "\n",
    "Try sampling from your model with different temperature settings\n",
    "(e.g. from 0.8 to 5). Make sure to sample multiple times per setting.\n",
    "Since we only trained the model on a single sequence, we won't see\n",
    "the full effect of the temperature parameter yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 : @whin5 nice one \n",
      "1.5 : @whin5 nice one \n",
      "0.5 : @whin5 nice one \n",
      "0.25 : @whin5 nice one \n",
      "2 : @whin5 noce no@whinwh\n",
      "3 : @whiii whe nc<BOS>e \n",
      "4 : @cen o<BOS>@whwhin5ce<BOS> oehe \n",
      "5 : h<BOS>oineownn\n"
     ]
    }
   ],
   "source": [
    "\"input is <BOS>\"\n",
    "print('1.0 :',sample_sequence(model, temperature=1.0))\n",
    "print('1.5 :',sample_sequence(model, temperature=1.5))\n",
    "print('0.5 :',sample_sequence(model, temperature=0.5))\n",
    "print('0.25 :',sample_sequence(model, temperature=0.25))\n",
    "print('2 :', sample_sequence(model, temperature=2.0))\n",
    "print('3 :',sample_sequence(model, temperature=3.0))\n",
    "print('4 :',sample_sequence(model, temperature=4.0))\n",
    "print('5 :',sample_sequence(model, temperature=5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, the output of the calls to the `sample_sequence` function\n",
    "assures us that our training code looks reasonable, and we can\n",
    "proceed to training on our full dataset!\n",
    "\n",
    "## Training the Tweet Generator\n",
    "\n",
    "For the actual training, let's use `torchtext` so that we can use\n",
    "the `BucketIterator` to make batches. Like in Lab 5, we'll create a \n",
    "`torchtext.data.Field` to use `torchtext` to read the CSV file, and convert\n",
    "characters into indices. The object has convient parameters to specify\n",
    "the BOS and EOS tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchtext\n",
    "\n",
    "text_field = torchtext.data.Field(sequential=True,      # text sequence\n",
    "                                  tokenize=lambda x: x, # because are building a character-RNN\n",
    "                                  include_lengths=True, # to track the length of sequences, for batching\n",
    "                                  batch_first=True,\n",
    "                                  use_vocab=True,       # to turn each character into an integer index\n",
    "                                  init_token=\"<BOS>\",   # BOS token\n",
    "                                  eos_token=\"<EOS>\")    # EOS token\n",
    "\n",
    "fields = [('text', text_field)]\n",
    "tweets = torchtext.data.TabularDataset(\"happy.csv\", \"csv\", fields)\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the version of `vocab_stoi` and `vocab_itos` that torchtext provides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_field.build_vocab(tweets)\n",
    "vocab_stoi = text_field.vocab.stoi # so we don't have to rewrite sample_sequence\n",
    "vocab_itos = text_field.vocab.itos # so we don't have to rewrite sample_sequence\n",
    "vocab_size = len(text_field.vocab.itos)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk> 0\n",
      "<pad> 1\n",
      "<BOS> 2\n",
      "<EOS> 3\n",
      "  4\n",
      "e 5\n",
      "o 6\n",
      "t 7\n",
      "a 8\n",
      "i 9\n"
     ]
    }
   ],
   "source": [
    "for k, v in vocab_stoi.items():\n",
    "    if v >= 10: break\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just verify that the `BucketIterator` works as expected, but start with batch_size of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([52])\n",
      "torch.Size([1, 52])\n"
     ]
    }
   ],
   "source": [
    "data_iter = torchtext.data.BucketIterator(tweets, \n",
    "                                          batch_size=1,\n",
    "                                          sort_key=lambda x: len(x.text),\n",
    "                                          sort_within_batch=True)\n",
    "for (tweet, lengths), label in data_iter:\n",
    "    print(label)   # should be None\n",
    "    print(lengths) # contains the length of the tweet(s) in batch\n",
    "    print(tweet.shape) # should be [1, max(length)]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (e)\n",
    "\n",
    "Modify the training code to account for batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, batch_size=1, num_epochs=1, lr=0.001, print_every=100):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    it = 0\n",
    "    \n",
    "    data_iter = torchtext.data.BucketIterator(data,\n",
    "                                              batch_size=batch_size,\n",
    "                                              sort_key=lambda x: len(x.text),\n",
    "                                              sort_within_batch=True)\n",
    "    for e in range(num_epochs):\n",
    "        # get training set\n",
    "        avg_loss = 0\n",
    "        for (tweet, lengths), label in data_iter:\n",
    "            target = tweet[:, 1:] # TODO\n",
    "            inp = tweet[:, :-1] # TODO\n",
    "            # cleanup\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            output, _ = model(inp) # TODO\n",
    "            loss = criterion(output.reshape(-1, vocab_size), target.reshape(-1)) # TODO\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_loss += loss\n",
    "            it += 1 # increment iteration count\n",
    "            if it % print_every == 0:\n",
    "                print(\"[Iter %d] Loss %f\" % (it+1, float(avg_loss/print_every)))\n",
    "                print(\"    \" + sample_sequence(model, 140, 0.8))\n",
    "                avg_loss = 0\n",
    "\n",
    "model = TextGenerator(vocab_size, 128, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (f)\n",
    "\n",
    "Train your model. This model might take a looong time too train. However, the model\n",
    "learns very quickly that many tweets begin with `@`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 101] Loss 3.188071\n",
      "    @ind Iot han.. Yat ntenil... win.aother thing Won anucoy wahiven  hating tusss atthaye  oun wat ar daa an Ther sra hing Ee pot wingil.. \n",
      "[Iter 201] Loss 2.907591\n",
      "    @jbai bea\n",
      "[Iter 301] Loss 2.754739\n",
      "    @Rekiorninin ge anthe ons &qukinings ap stoplerulk\n",
      "[Iter 401] Loss 2.723065\n",
      "    mingerd\n",
      "[Iter 501] Loss 2.642127\n",
      "    @LOM Ohs \n",
      "[Iter 601] Loss 2.604728\n",
      "    @lazby yout gooda\n",
      "[Iter 701] Loss 2.594453\n",
      "    @guata to the \n",
      "[Iter 801] Loss 2.536493\n",
      "    Ho\n",
      "[Iter 901] Loss 2.562485\n",
      "    Hi\n",
      "[Iter 1001] Loss 2.488791\n",
      "    iiah bay and\n",
      "[Iter 1101] Loss 2.469344\n",
      "    @icarmorter bob ah er bood thelly \n",
      "[Iter 1201] Loss 2.444599\n",
      "    ok st;y wooken the so to is hawing ther come ar tot chak... \n",
      "[Iter 1301] Loss 2.470910\n",
      "    @makeardaytt1 be I coon't you selyoo\n",
      "[Iter 1401] Loss 2.520337\n",
      "    Ems\n",
      "[Iter 1501] Loss 2.604313\n",
      "    It's hat the  cot al diss arffr the rearse \n",
      "[Iter 1601] Loss 2.457621\n",
      "    @sagaster (TH Hah 4' hudtywerse to Some co ceand \n",
      "[Iter 1701] Loss 2.536667\n",
      "    I for exspechixorn you be fuges out haver ware gummushot save al good \n",
      "[Iter 1801] Loss 2.494597\n",
      "    @SJeliea withed Thine.......................................................................................................................\n",
      "[Iter 1901] Loss 2.511606\n",
      "    Loank anar the me tomoir mes sure fougu love  rasing to woo peornang ferh ye ing to me \n",
      "[Iter 2001] Loss 2.468259\n",
      "    @VEYESOON!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "[Iter 2101] Loss 2.460139\n",
      "    My hoterd Buorning new have the aning   coferting onas a ling\n",
      "[Iter 2201] Loss 2.425492\n",
      "    @Beend11 \n",
      "[Iter 2301] Loss 2.423676\n",
      "    @Rooomhhave yow my ining al gre ong thanks me awe my twitt int time ware lou your ay lins tomon be Morning lead \n",
      "[Iter 2401] Loss 2.417207\n",
      "    oh you and weep\n",
      "[Iter 2501] Loss 2.469757\n",
      "    @patttlonjost therous thelly Nes \n",
      "[Iter 2601] Loss 2.482789\n",
      "    @kanpshen haseves to yash\n",
      "[Iter 2701] Loss 2.460629\n",
      "    just thx #Junjher for Frarety \n",
      "[Iter 2801] Loss 2.387992\n",
      "    uled  - me that a we the sose all day #S1112 welk an work\n",
      "[Iter 2901] Loss 2.391930\n",
      "    @shsyosha @iaha lho \n",
      "[Iter 3001] Loss 2.404782\n",
      "    @Haughitts stelly stere 2 route Ro! Yehea you thes  Leate to your all gett moe out my bay it be ye you hister to sor Cure wy a with the 4 do\n",
      "[Iter 3101] Loss 2.438700\n",
      "    Yoo Hears luck in stary. \n",
      "[Iter 3201] Loss 2.350633\n",
      "    @siahth comerse and the bay\n",
      "[Iter 3301] Loss 2.475886\n",
      "    Wo! \n",
      "[Iter 3401] Loss 2.476511\n",
      "    @jinthuyott to good 'mp good a be what i shoopele the supparing ..&quadnpiow. \n",
      "[Iter 3501] Loss 2.416734\n",
      "    bes you lit the \n",
      "[Iter 3601] Loss 2.359318\n",
      "    @ksp6yso frmarkink sous out wast just\n",
      "[Iter 3701] Loss 2.430739\n",
      "    Okay\n",
      "[Iter 3801] Loss 2.405998\n",
      "    @aslennnninelle  ando  hoome\n",
      "[Iter 3901] Loss 2.391674\n",
      "    @raddellynnlome   No todayy liceani downing.   home  veepto sell cupe \n",
      "[Iter 4001] Loss 2.373287\n",
      "    @jnassever Reat\n",
      "[Iter 4101] Loss 2.390781\n",
      "    @fuenyendindiest  have so ans is and gone a eames \n",
      "[Iter 4201] Loss 2.413446\n",
      "    @Shaille Sooooo morning ooit brom here this even me bige got  everry fhul a chomered  an't do sting the becad getsiating furme on worgen \n",
      "[Iter 4301] Loss 2.327035\n",
      "    Soo\n",
      "[Iter 4401] Loss 2.439122\n",
      "    @ticataixall\n",
      "[Iter 4501] Loss 2.359548\n",
      "    @atievhe_shackerry9 Joust to the frupe\n",
      "[Iter 4601] Loss 2.362676\n",
      "    @Suy__SAILE ...... Nea 'lrea pong fried a sep. I har and Itw worry is the bcood to migh is a mis  the ponaties on I parin a price in to like\n",
      "[Iter 4701] Loss 2.369874\n",
      "    @aherePutesatied OOOKKKWOOO!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! \n",
      "[Iter 4801] Loss 2.362488\n",
      "    @flllotombough gunnc... \n",
      "[Iter 4901] Loss 2.427550\n",
      "    @SbedICSTTTE  I'm me am bad mile ar the tho have an worry!!!!!!!!!!!!!!!! Steeftast!!!!!!!!!!!!!!!!!!! I am \n",
      "[Iter 5001] Loss 2.370921\n",
      "    @ilasbzzz in Lod and you by ilas ut @campy ell to row\n",
      "[Iter 5101] Loss 2.403608\n",
      "    @Jeleises some \n",
      "[Iter 5201] Loss 2.388005\n",
      "    I beeai goood\n",
      "[Iter 5301] Loss 2.383203\n",
      "    @Alids Yayuss good goood be-cing in mast with hahaaaal haha\n",
      "[Iter 5401] Loss 2.358783\n",
      "    @adwanice tamanerous viday to week Vetping and over week later and The Spem to pleank do \n",
      "[Iter 5501] Loss 2.374835\n",
      "    @Yynmmfuch The Fenass a next rest a that! Seet Some as to was it's trest loat! I'm be a get you to weet that's i lill to = tom spithemcifmmo\n",
      "[Iter 5601] Loss 2.335281\n",
      "    Morny shed me home the oks. \n",
      "[Iter 5701] Loss 2.374692\n",
      "    @juninjuch O? THER Good Bo morring and achoto Bechicon\n",
      "[Iter 5801] Loss 2.296941\n",
      "    @virhirking Good \n",
      "[Iter 5901] Loss 2.309270\n",
      "    @ yay cupe watwere? torina the my cuncze Deals I nahakion awd the make yous have \n",
      "[Iter 6001] Loss 2.352524\n",
      "    @zibackicking Thanks  I hat goud a about the 'm likes Thanks telly wahkt  (I waited for to sning to get a wat eam nap appled dais Thanks for\n",
      "[Iter 6101] Loss 2.396966\n",
      "    some not... follow.   LOVETPL! \n",
      "[Iter 6201] Loss 2.416800\n",
      "    @severien entipe up \n",
      "[Iter 6301] Loss 2.430811\n",
      "    @mmmysel Chig my the hopes\n",
      "[Iter 6401] Loss 2.347308\n",
      "    Good usal\n",
      "[Iter 6501] Loss 2.503448\n",
      "    @cherappy_chrda HAHAH HAHAH Heade's say a pean hard a pled &appasing the day happy hear wello\n",
      "[Iter 6601] Loss 2.334325\n",
      "    @merotosumarend Just afeen such!!!!! &ut; baght a borly yeer hit bus you teen Jould was for a for nowe hahaaarl work how a ga \n",
      "[Iter 6701] Loss 2.421630\n",
      "    Goodmum &amp; Ast the oling seede do espass the not fing dat am for withing myzine \n",
      "[Iter 6801] Loss 2.346423\n",
      "    @viekiessing \n",
      "[Iter 6901] Loss 2.473204\n",
      "    @#taiplouss boow me exams they\n",
      "[Iter 7001] Loss 2.380911\n",
      "    @hhikkpeday hat apparl mis for to fitice the Mow\n",
      "[Iter 7101] Loss 2.427099\n",
      "    @wat The #thissisch I'm it's All \n",
      "[Iter 7201] Loss 2.375174\n",
      "    @Jeransesf\n",
      "[Iter 7301] Loss 2.420829\n",
      "    @juseheromes I know of Turssice \n",
      "[Iter 7401] Loss 2.346287\n",
      "    @DF4ack \n",
      "[Iter 7501] Loss 2.344191\n",
      "    @MiggRARERIS NEd. No\n",
      "[Iter 7601] Loss 2.352946\n",
      "    @chandista  horning\n",
      "[Iter 7701] Loss 2.341559\n",
      "    @Mymatick to picked. at patender and a mad is work the Yeop.  ?\n",
      "[Iter 7801] Loss 2.406976\n",
      "    @ellyFx99 Anose de it time \n",
      "[Iter 7901] Loss 2.338104\n",
      "    neallly  No\n",
      "[Iter 8001] Loss 2.358193\n",
      "    @vephderr @xIssser Ka cmeres me good is wast so the weechomes. \n",
      "[Iter 8101] Loss 2.428052\n",
      "    sthrous Morning to this ulfesy friem for my beading booking sayshin beeping the with UST I st. I cars\n",
      "[Iter 8201] Loss 2.434778\n",
      "    @cereadaney ;hand cae you neeel   on comny better\n",
      "[Iter 8301] Loss 2.356100\n",
      "    Off the you love at next twitter thery just ocass weeks tomorirgoot thenp with heresy! Sup theng as see \n",
      "[Iter 8401] Loss 2.390086\n",
      "    @buoppttil Good shope my waind thing \n",
      "[Iter 8501] Loss 2.329520\n",
      "    Hami! Good and you red and with feerst trealluld tedd my choce y boy gery for a lishher Plasted Wordous! \n",
      "[Iter 8601] Loss 2.296540\n",
      "    @christying Oh\n",
      "[Iter 8701] Loss 2.298390\n",
      "    @Thormz @Estiess.o \n",
      "[Iter 8801] Loss 2.385314\n",
      "    @tylitisted Good soon wat pic. I hall it \n",
      "[Iter 8901] Loss 2.358305\n",
      "    @Calenbelarraug Yo to some\n",
      "[Iter 9001] Loss 2.350782\n",
      "    @Haalall\n",
      "[Iter 9101] Loss 2.350909\n",
      "    Mording to wetter BMan! seap it cann tome \n",
      "[Iter 9201] Loss 2.341733\n",
      "    @jhehirooinding Cool Jank my expert\n",
      "[Iter 9301] Loss 2.348815\n",
      "    Morwed\n",
      "[Iter 9401] Loss 2.395100\n",
      "    @shahi heht well sidy  the ploss good for sfucase the the the time right in turndess the new the a good  jubus in aries\n",
      "[Iter 9501] Loss 2.299186\n",
      "    Ling......... \n",
      "[Iter 9601] Loss 2.375981\n",
      "    ou\n",
      "[Iter 9701] Loss 2.300920\n",
      "    @kheshelfor pxorlyz on like the d my phown\n",
      "[Iter 9801] Loss 2.356755\n",
      "    @REMEWOALLYWOOOOOOOB Lol\n",
      "[Iter 9901] Loss 2.335804\n",
      "    @sayygelgrol Yough.days shorning!!!!!! a for ceats a wat that's so\n",
      "[Iter 10001] Loss 2.284412\n",
      "    @ruddEnink The loling Thel 16 137 soll for mound my se\n",
      "[Iter 10101] Loss 2.297940\n",
      "    @lagoutella Cart love it sumh\n",
      "[Iter 10201] Loss 2.365999\n",
      "    im just my tick my albazs the ;) going.\n",
      "[Iter 10301] Loss 2.371633\n",
      "    che in Farly \n",
      "[Iter 10401] Loss 2.343944\n",
      "    sowentated listexs a ke mm.all! wht\n",
      "[Iter 10501] Loss 2.356248\n",
      "    @JelianBadAu charff how Noh @surnerryy yeahater\n",
      "[Iter 10601] Loss 2.319066\n",
      "    @Okerof Hatbard did Long toning. \n",
      "[Iter 10701] Loss 2.399643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    man a abrest sice \n",
      "[Iter 10801] Loss 2.384819\n",
      "    @mitt_hrirem soool\n",
      "[Iter 10901] Loss 2.398401\n",
      "    @gics wold is my the till a\n",
      "[Iter 11001] Loss 2.342171\n",
      "    not a fat\n",
      "[Iter 11101] Loss 2.344911\n",
      "    @sanheldissne sone \n",
      "[Iter 11201] Loss 2.392545\n",
      "    @bedelR Thouling hisping tomore \n",
      "[Iter 11301] Loss 2.351994\n",
      "    @AKandav Have Heyian lek havad! \n",
      "[Iter 11401] Loss 2.337790\n",
      "    @ziceferryleeptersisca the kipes that funny\n",
      "[Iter 11501] Loss 2.359103\n",
      "    @Righirst 155555\n",
      "[Iter 11601] Loss 2.363757\n",
      "    @areed my laheshot hist than nevioh!   That's eahing \n",
      "[Iter 11701] Loss 2.342891\n",
      "    @zisoi toooooo missy today  Thatiar Mocom! \n",
      "[Iter 11801] Loss 2.285158\n",
      "    I heh as all si going\n",
      "[Iter 11901] Loss 2.398196\n",
      "    @pannnyane_chimi canhigre\n",
      "[Iter 12001] Loss 2.333675\n",
      "    Gothing. \n",
      "[Iter 12101] Loss 2.310496\n",
      "    @JillMymienEan Gorning \n",
      "[Iter 12201] Loss 2.333087\n",
      "    Off on\n",
      "[Iter 12301] Loss 2.355722\n",
      "    Yo i arite yourding to and cool kind night \n",
      "[Iter 12401] Loss 2.369042\n",
      "    @vertlear\n",
      "[Iter 12501] Loss 2.327537\n",
      "    @caitonday  Thage past. \n",
      "[Iter 12601] Loss 2.354976\n",
      "    @DayLee haha\n",
      "[Iter 12701] Loss 2.260625\n",
      "    @cagaodas Than stil stenter  \n",
      "[Iter 12801] Loss 2.270280\n",
      "    cationing wnould! \n",
      "[Iter 12901] Loss 2.346893\n",
      "    Matchores onested\n",
      "[Iter 13001] Loss 2.369344\n",
      "    @swerA9  mest the hern't tomes wear just thinater! \n",
      "[Iter 13101] Loss 2.389972\n",
      "    @LigEAND HeheOker on heake\n",
      "[Iter 13201] Loss 2.374366\n",
      "    @Botterten  I'm that I good for thanks new pastersad for! \n",
      "[Iter 13301] Loss 2.294807\n",
      "    @Dfulil hare of work willcked is tyays the ly good miss is the can's that happy to hehe gon't good morning in the http://byutiner. I morning\n",
      "[Iter 13401] Loss 2.273528\n",
      "    hat I'm cating todar a a \n",
      "[Iter 13501] Loss 2.357260\n",
      "    @R_MagTiebit Loome day\n",
      "[Iter 13601] Loss 2.285611\n",
      "    @sarDess Ah\n",
      "[Iter 13701] Loss 2.254384\n",
      "    Well\n",
      "[Iter 13801] Loss 2.343735\n",
      "    Good morning \n",
      "[Iter 13901] Loss 2.310332\n",
      "    @wintiontie Just maving \n",
      "[Iter 14001] Loss 2.348826\n",
      "    @butiqas is eska the de my everyone mave in @chorrylettar Li't Ntrid.\n",
      "[Iter 14101] Loss 2.365714\n",
      "    @dubalresJa it dirl i home! I cank \n",
      "[Iter 14201] Loss 2.366581\n",
      "    @danling @fanchanic Saring tritting aftery is it weeks of yesp\n",
      "[Iter 14301] Loss 2.390688\n",
      "    Thanks\n",
      "[Iter 14401] Loss 2.284231\n",
      "    @Marbwangeryyyyyy donday tosic theyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy rack my stust the the can to it to veryone \n",
      "[Iter 14501] Loss 2.346292\n",
      "    @WeleotitlY scofe ray you a day andery yous stufa and you my cons one  \n",
      "[Iter 14601] Loss 2.252177\n",
      "    Winkink you ly is \n",
      "[Iter 14701] Loss 2.460029\n",
      "    @TeristWEKL it can to was! don't on didto home lorlies of podery band you o woooorry of elled to ome busome apy for teid is or take gree sho\n",
      "[Iter 14801] Loss 2.423584\n",
      "    @sabrurressin to dood sath tim dow is if o weet\n",
      "[Iter 14901] Loss 2.372040\n",
      "    @Jellaso Yum you \n",
      "[Iter 15001] Loss 2.438895\n",
      "    @jeldajew Yeay!  I'man hm! Linateds  #Strypenicames got to dow a know\n",
      "[Iter 15101] Loss 2.340713\n",
      "    @JustWn_Havazo the That so Dood able abten for  wee\n",
      "[Iter 15201] Loss 2.359232\n",
      "    Bakey and Good mint a goow undelly \n",
      "[Iter 15301] Loss 2.359527\n",
      "    @dacjeammes love of love the ds  http://twwitplets..........................to/.b/4 IC \n",
      "[Iter 15401] Loss 2.404735\n",
      "    @suraws looool. That day! \n",
      "[Iter 15501] Loss 2.325099\n",
      "    it' therome and a nidit. \n",
      "[Iter 15601] Loss 2.405591\n",
      "    @donry068 eams you dearly thingedine freats to the way \n",
      "[Iter 15701] Loss 2.312596\n",
      "    @Jsassenensove - up happe courds chou8dyt  thankse to tome the courd twitter up gree to today.. wess the the Prom pree swere a grat soo bigt\n",
      "[Iter 15801] Loss 2.392497\n",
      "    @AntyyI Hey\n",
      "[Iter 15901] Loss 2.355680\n",
      "    @stonly__IGed 755\n",
      "[Iter 16001] Loss 2.501047\n",
      "    and agorning Bow @guyguoster arou hard to grow aww Mon  on findem one to 2 done day morninder to of a  can IN happy dayyyy day to seee a kii\n",
      "[Iter 16101] Loss 2.304912\n",
      "    @Ntileweet Dodecter soon! Hoper'le Hows thinst\n",
      "[Iter 16201] Loss 2.308570\n",
      "    @6bit's got all fir hoper ache awaake it!! \n",
      "[Iter 16301] Loss 2.341783\n",
      "    Ext\n",
      "[Iter 16401] Loss 2.357452\n",
      "    Every csantingit ayk \n",
      "[Iter 16501] Loss 2.402235\n",
      "    @Corcaeso Yuv'?????????? It have haha  alacho \n",
      "[Iter 16601] Loss 2.307756\n",
      "    @mkilling is it todd not soon to play home yep \n",
      "[Iter 16701] Loss 2.368590\n",
      "    @mying That arle to tai seready Ehh\n",
      "[Iter 16801] Loss 2.383185\n",
      "    @motly099 now \n",
      "[Iter 16901] Loss 2.364781\n",
      "    @Jramarltounnand @Liddappenplorkinga you get people a storling it's muve at time work you pleass the a   good you a pan is go per it's my mo\n",
      "[Iter 17001] Loss 2.343063\n",
      "    @comrckied Chtperts \n",
      "[Iter 17101] Loss 2.354391\n",
      "    @shemebachs and now\n",
      "[Iter 17201] Loss 2.319422\n",
      "    @dnvataiand how ah\n",
      "[Iter 17301] Loss 2.356712\n",
      "    To tear you  \n",
      "[Iter 17401] Loss 2.319646\n",
      "    @gyning mupose book offfffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffful grus \n",
      "[Iter 17501] Loss 2.359944\n",
      "    and from on More I rad Sleat pp too who maacuert Deahhh\n",
      "[Iter 17601] Loss 2.301275\n",
      "    @loverereedy He stood a thanks the the cord the to weeks vice to weeely good fors you von? tweet teet good to the tham soo\n",
      "[Iter 17701] Loss 2.327167\n",
      "    herm \n",
      "[Iter 17801] Loss 2.307813\n",
      "    Going for my Lit sleping tomo? I wat is till be to bow pereeating\n",
      "[Iter 17901] Loss 2.369072\n",
      "    @bakabsidmxx got\n",
      "[Iter 18001] Loss 2.302714\n",
      "    Hand nida? \n",
      "[Iter 18101] Loss 2.352968\n",
      "    Tight! \n",
      "[Iter 18201] Loss 2.290002\n",
      "    ASting Siona to not haning afins and yeah\n",
      "[Iter 18301] Loss 2.412978\n",
      "    @imalodallooine @eatines Yup fready out and well\n",
      "[Iter 18401] Loss 2.372164\n",
      "    im couret und here? \n",
      "[Iter 18501] Loss 2.288446\n",
      "    @man @jaathankyamagh \n",
      "[Iter 18601] Loss 2.425722\n",
      "    @Cortiee77nice Tomer and forthot same to cong aprepperhyser. \n",
      "[Iter 18701] Loss 2.275424\n",
      "    doods ump\n",
      "[Iter 18801] Loss 2.256544\n",
      "    @mash and = baun  \n",
      "[Iter 18901] Loss 2.288880\n",
      "    @shanduce You ! \n",
      "[Iter 19001] Loss 2.316438\n",
      "    @Cupique Oha for you pette. have pare they some to pincenthing inse for curst homi helg anding uchi come to hunds wish hop\n",
      "[Iter 19101] Loss 2.347381\n",
      "    Reryfing \n",
      "[Iter 19201] Loss 2.330293\n",
      "    pock hahahl. \n",
      "[Iter 19301] Loss 2.364534\n",
      "    @Prumangney It the pene! \n",
      "[Iter 19401] Loss 2.359458\n",
      "    @katentheemc421hab the from it worke to to the the blaby tooooo in i was houtarter tomorrd ast you \n",
      "[Iter 19501] Loss 2.361087\n",
      "    @guargylead I got and I mesy pickers I from povie to good mak brick to be as\n",
      "[Iter 19601] Loss 2.292484\n",
      "    At\n",
      "[Iter 19701] Loss 2.335454\n",
      "    @Ibberancely shank sound it!! \n",
      "[Iter 19801] Loss 2.269804\n",
      "    MORY \n",
      "[Iter 19901] Loss 2.342175\n",
      "    @Fikepellosolong  Herrow the raily u coom beer your fing nogh\n",
      "[Iter 20001] Loss 2.283606\n",
      "    @morinightrits UTpG &lt;i hanshe \n"
     ]
    }
   ],
   "source": [
    "train(model, tweets, batch_size=1, num_epochs=1, lr=0.004, print_every=100)\n",
    "#train(model, tweets, batch_size=32, num_epochs=1, lr=0.004, print_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (g)\n",
    "\n",
    "Try generating some sequences using your model. Vary your temperature settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sample_sequence(model, temperature=0.8))\n",
    "#print(sample_sequence(model, temperature=0.8))\n",
    "#print(sample_sequence(model, temperature=1.0))\n",
    "#print(sample_sequence(model, temperature=1.0))\n",
    "#print(sample_sequence(model, temperature=1.5))\n",
    "#print(sample_sequence(model, temperature=1.5))\n",
    "#print(sample_sequence(model, temperature=2.0))\n",
    "#print(sample_sequence(model, temperature=2.0))\n",
    "#print(sample_sequence(model, temperature=5.0))\n",
    "#print(sample_sequence(model, temperature=5.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
