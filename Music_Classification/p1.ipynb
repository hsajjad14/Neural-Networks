{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC321H5 Project 1. Music Millenium Classification\n",
    "\n",
    "**Deadline**: Thursday, Jan. 30, by 9pm\n",
    "\n",
    "**Submission**: Submit a PDF export of the completed notebook. \n",
    "\n",
    "**Late Submission**: Please see the syllabus for the late submission criteria.\n",
    "\n",
    "To celebrate the start of a new decade, we will build models to predict which\n",
    "**century** a piece of music was released.  We will be using the \"YearPredictionMSD Data Set\"\n",
    "based on the Million Song Dataset. The data is available to download from the UCI \n",
    "Machine Learning Repository. Here are some links about the data:\n",
    "\n",
    "- https://archive.ics.uci.edu/ml/datasets/yearpredictionmsd\n",
    "- http://millionsongdataset.com/pages/tasks-demos/#yearrecognition\n",
    "\n",
    "## Question 1. Data\n",
    "\n",
    "Start by setting up a Google Colab notebook in which to do your work.\n",
    "If you are working with a partner, you might find this link helpful:\n",
    "\n",
    "- https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb\n",
    "\n",
    "The recommended way to work together is pair coding, where you and your partner are sitting together and writing code together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your notebook is set up, we can load the data into the notebook. The code below provides\n",
    "two ways of loading the data: directly from the internet, or through mounting Google Drive.\n",
    "The first method is easier but slower, and the second method is a bit involved at first, but\n",
    "can save you time later on. You will need to mount Google Drive for later assignments, so we recommend\n",
    "figuring how to do that now.\n",
    "\n",
    "Here are some resources to help you get started:\n",
    "\n",
    "- http.://colab.research.google.com/notebooks/io.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_drive = False\n",
    "\n",
    "# if not load_from_drive:\n",
    "#   csv_path = \"http://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip\"\n",
    "# else:\n",
    "#   from google.colab import drive\n",
    "#   drive.mount('/content/gdrive')\n",
    "#   csv_path = '/content/drive/My Drive/YearPredictionMSD.txt.zip' # TODO - UPDATE ME!\n",
    "\n",
    "# this is for the google colab ^\n",
    "\n",
    "# already have it in folder\n",
    "csv_path = 'YearPredictionMSD_folder/YearPredictionMSD.txt'\n",
    "\n",
    "t_label = [\"year\"]\n",
    "x_labels = [\"var%d\" % i for i in range(1, 91)]\n",
    "df = pandas.read_csv(csv_path, names=t_label + x_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is loaded to your Colab notebook, you should be able to display the Pandas\n",
    "DataFrame `df` as a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>var1</th>\n",
       "      <th>var2</th>\n",
       "      <th>var3</th>\n",
       "      <th>var4</th>\n",
       "      <th>var5</th>\n",
       "      <th>var6</th>\n",
       "      <th>var7</th>\n",
       "      <th>var8</th>\n",
       "      <th>var9</th>\n",
       "      <th>...</th>\n",
       "      <th>var81</th>\n",
       "      <th>var82</th>\n",
       "      <th>var83</th>\n",
       "      <th>var84</th>\n",
       "      <th>var85</th>\n",
       "      <th>var86</th>\n",
       "      <th>var87</th>\n",
       "      <th>var88</th>\n",
       "      <th>var89</th>\n",
       "      <th>var90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001</td>\n",
       "      <td>49.94357</td>\n",
       "      <td>21.47114</td>\n",
       "      <td>73.07750</td>\n",
       "      <td>8.74861</td>\n",
       "      <td>-17.40628</td>\n",
       "      <td>-13.09905</td>\n",
       "      <td>-25.01202</td>\n",
       "      <td>-12.23257</td>\n",
       "      <td>7.83089</td>\n",
       "      <td>...</td>\n",
       "      <td>13.01620</td>\n",
       "      <td>-54.40548</td>\n",
       "      <td>58.99367</td>\n",
       "      <td>15.37344</td>\n",
       "      <td>1.11144</td>\n",
       "      <td>-23.08793</td>\n",
       "      <td>68.40795</td>\n",
       "      <td>-1.82223</td>\n",
       "      <td>-27.46348</td>\n",
       "      <td>2.26327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>48.73215</td>\n",
       "      <td>18.42930</td>\n",
       "      <td>70.32679</td>\n",
       "      <td>12.94636</td>\n",
       "      <td>-10.32437</td>\n",
       "      <td>-24.83777</td>\n",
       "      <td>8.76630</td>\n",
       "      <td>-0.92019</td>\n",
       "      <td>18.76548</td>\n",
       "      <td>...</td>\n",
       "      <td>5.66812</td>\n",
       "      <td>-19.68073</td>\n",
       "      <td>33.04964</td>\n",
       "      <td>42.87836</td>\n",
       "      <td>-9.90378</td>\n",
       "      <td>-32.22788</td>\n",
       "      <td>70.49388</td>\n",
       "      <td>12.04941</td>\n",
       "      <td>58.43453</td>\n",
       "      <td>26.92061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001</td>\n",
       "      <td>50.95714</td>\n",
       "      <td>31.85602</td>\n",
       "      <td>55.81851</td>\n",
       "      <td>13.41693</td>\n",
       "      <td>-6.57898</td>\n",
       "      <td>-18.54940</td>\n",
       "      <td>-3.27872</td>\n",
       "      <td>-2.35035</td>\n",
       "      <td>16.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>3.03800</td>\n",
       "      <td>26.05866</td>\n",
       "      <td>-50.92779</td>\n",
       "      <td>10.93792</td>\n",
       "      <td>-0.07568</td>\n",
       "      <td>43.20130</td>\n",
       "      <td>-115.00698</td>\n",
       "      <td>-0.05859</td>\n",
       "      <td>39.67068</td>\n",
       "      <td>-0.66345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001</td>\n",
       "      <td>48.24750</td>\n",
       "      <td>-1.89837</td>\n",
       "      <td>36.29772</td>\n",
       "      <td>2.58776</td>\n",
       "      <td>0.97170</td>\n",
       "      <td>-26.21683</td>\n",
       "      <td>5.05097</td>\n",
       "      <td>-10.34124</td>\n",
       "      <td>3.55005</td>\n",
       "      <td>...</td>\n",
       "      <td>34.57337</td>\n",
       "      <td>-171.70734</td>\n",
       "      <td>-16.96705</td>\n",
       "      <td>-46.67617</td>\n",
       "      <td>-12.51516</td>\n",
       "      <td>82.58061</td>\n",
       "      <td>-72.08993</td>\n",
       "      <td>9.90558</td>\n",
       "      <td>199.62971</td>\n",
       "      <td>18.85382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001</td>\n",
       "      <td>50.97020</td>\n",
       "      <td>42.20998</td>\n",
       "      <td>67.09964</td>\n",
       "      <td>8.46791</td>\n",
       "      <td>-15.85279</td>\n",
       "      <td>-16.81409</td>\n",
       "      <td>-12.48207</td>\n",
       "      <td>-9.37636</td>\n",
       "      <td>12.63699</td>\n",
       "      <td>...</td>\n",
       "      <td>9.92661</td>\n",
       "      <td>-55.95724</td>\n",
       "      <td>64.92712</td>\n",
       "      <td>-17.72522</td>\n",
       "      <td>-1.49237</td>\n",
       "      <td>-7.50035</td>\n",
       "      <td>51.76631</td>\n",
       "      <td>7.88713</td>\n",
       "      <td>55.66926</td>\n",
       "      <td>28.74903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   year      var1      var2      var3      var4      var5      var6      var7  \\\n",
       "0  2001  49.94357  21.47114  73.07750   8.74861 -17.40628 -13.09905 -25.01202   \n",
       "1  2001  48.73215  18.42930  70.32679  12.94636 -10.32437 -24.83777   8.76630   \n",
       "2  2001  50.95714  31.85602  55.81851  13.41693  -6.57898 -18.54940  -3.27872   \n",
       "3  2001  48.24750  -1.89837  36.29772   2.58776   0.97170 -26.21683   5.05097   \n",
       "4  2001  50.97020  42.20998  67.09964   8.46791 -15.85279 -16.81409 -12.48207   \n",
       "\n",
       "       var8      var9  ...     var81      var82     var83     var84     var85  \\\n",
       "0 -12.23257   7.83089  ...  13.01620  -54.40548  58.99367  15.37344   1.11144   \n",
       "1  -0.92019  18.76548  ...   5.66812  -19.68073  33.04964  42.87836  -9.90378   \n",
       "2  -2.35035  16.07017  ...   3.03800   26.05866 -50.92779  10.93792  -0.07568   \n",
       "3 -10.34124   3.55005  ...  34.57337 -171.70734 -16.96705 -46.67617 -12.51516   \n",
       "4  -9.37636  12.63699  ...   9.92661  -55.95724  64.92712 -17.72522  -1.49237   \n",
       "\n",
       "      var86      var87     var88      var89     var90  \n",
       "0 -23.08793   68.40795  -1.82223  -27.46348   2.26327  \n",
       "1 -32.22788   70.49388  12.04941   58.43453  26.92061  \n",
       "2  43.20130 -115.00698  -0.05859   39.67068  -0.66345  \n",
       "3  82.58061  -72.08993   9.90558  199.62971  18.85382  \n",
       "4  -7.50035   51.76631   7.88713   55.66926  28.74903  \n",
       "\n",
       "[5 rows x 91 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up our data for classification, we'll use the \"year\" field to represent\n",
    "whether a song was released in the 20-th century. In our case `df[\"year\"]` will be 1 if\n",
    "the year was released after 2000, and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"year\"] = df[\"year\"].map(lambda x: int(x > 2000))\n",
    "# if year > 2000 set it to 1, else keep it as 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>var1</th>\n",
       "      <th>var2</th>\n",
       "      <th>var3</th>\n",
       "      <th>var4</th>\n",
       "      <th>var5</th>\n",
       "      <th>var6</th>\n",
       "      <th>var7</th>\n",
       "      <th>var8</th>\n",
       "      <th>var9</th>\n",
       "      <th>...</th>\n",
       "      <th>var81</th>\n",
       "      <th>var82</th>\n",
       "      <th>var83</th>\n",
       "      <th>var84</th>\n",
       "      <th>var85</th>\n",
       "      <th>var86</th>\n",
       "      <th>var87</th>\n",
       "      <th>var88</th>\n",
       "      <th>var89</th>\n",
       "      <th>var90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>49.94357</td>\n",
       "      <td>21.47114</td>\n",
       "      <td>73.07750</td>\n",
       "      <td>8.74861</td>\n",
       "      <td>-17.40628</td>\n",
       "      <td>-13.09905</td>\n",
       "      <td>-25.01202</td>\n",
       "      <td>-12.23257</td>\n",
       "      <td>7.83089</td>\n",
       "      <td>...</td>\n",
       "      <td>13.01620</td>\n",
       "      <td>-54.40548</td>\n",
       "      <td>58.99367</td>\n",
       "      <td>15.37344</td>\n",
       "      <td>1.11144</td>\n",
       "      <td>-23.08793</td>\n",
       "      <td>68.40795</td>\n",
       "      <td>-1.82223</td>\n",
       "      <td>-27.46348</td>\n",
       "      <td>2.26327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>48.73215</td>\n",
       "      <td>18.42930</td>\n",
       "      <td>70.32679</td>\n",
       "      <td>12.94636</td>\n",
       "      <td>-10.32437</td>\n",
       "      <td>-24.83777</td>\n",
       "      <td>8.76630</td>\n",
       "      <td>-0.92019</td>\n",
       "      <td>18.76548</td>\n",
       "      <td>...</td>\n",
       "      <td>5.66812</td>\n",
       "      <td>-19.68073</td>\n",
       "      <td>33.04964</td>\n",
       "      <td>42.87836</td>\n",
       "      <td>-9.90378</td>\n",
       "      <td>-32.22788</td>\n",
       "      <td>70.49388</td>\n",
       "      <td>12.04941</td>\n",
       "      <td>58.43453</td>\n",
       "      <td>26.92061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>50.95714</td>\n",
       "      <td>31.85602</td>\n",
       "      <td>55.81851</td>\n",
       "      <td>13.41693</td>\n",
       "      <td>-6.57898</td>\n",
       "      <td>-18.54940</td>\n",
       "      <td>-3.27872</td>\n",
       "      <td>-2.35035</td>\n",
       "      <td>16.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>3.03800</td>\n",
       "      <td>26.05866</td>\n",
       "      <td>-50.92779</td>\n",
       "      <td>10.93792</td>\n",
       "      <td>-0.07568</td>\n",
       "      <td>43.20130</td>\n",
       "      <td>-115.00698</td>\n",
       "      <td>-0.05859</td>\n",
       "      <td>39.67068</td>\n",
       "      <td>-0.66345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>48.24750</td>\n",
       "      <td>-1.89837</td>\n",
       "      <td>36.29772</td>\n",
       "      <td>2.58776</td>\n",
       "      <td>0.97170</td>\n",
       "      <td>-26.21683</td>\n",
       "      <td>5.05097</td>\n",
       "      <td>-10.34124</td>\n",
       "      <td>3.55005</td>\n",
       "      <td>...</td>\n",
       "      <td>34.57337</td>\n",
       "      <td>-171.70734</td>\n",
       "      <td>-16.96705</td>\n",
       "      <td>-46.67617</td>\n",
       "      <td>-12.51516</td>\n",
       "      <td>82.58061</td>\n",
       "      <td>-72.08993</td>\n",
       "      <td>9.90558</td>\n",
       "      <td>199.62971</td>\n",
       "      <td>18.85382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50.97020</td>\n",
       "      <td>42.20998</td>\n",
       "      <td>67.09964</td>\n",
       "      <td>8.46791</td>\n",
       "      <td>-15.85279</td>\n",
       "      <td>-16.81409</td>\n",
       "      <td>-12.48207</td>\n",
       "      <td>-9.37636</td>\n",
       "      <td>12.63699</td>\n",
       "      <td>...</td>\n",
       "      <td>9.92661</td>\n",
       "      <td>-55.95724</td>\n",
       "      <td>64.92712</td>\n",
       "      <td>-17.72522</td>\n",
       "      <td>-1.49237</td>\n",
       "      <td>-7.50035</td>\n",
       "      <td>51.76631</td>\n",
       "      <td>7.88713</td>\n",
       "      <td>55.66926</td>\n",
       "      <td>28.74903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   year      var1      var2      var3      var4      var5      var6      var7  \\\n",
       "0     1  49.94357  21.47114  73.07750   8.74861 -17.40628 -13.09905 -25.01202   \n",
       "1     1  48.73215  18.42930  70.32679  12.94636 -10.32437 -24.83777   8.76630   \n",
       "2     1  50.95714  31.85602  55.81851  13.41693  -6.57898 -18.54940  -3.27872   \n",
       "3     1  48.24750  -1.89837  36.29772   2.58776   0.97170 -26.21683   5.05097   \n",
       "4     1  50.97020  42.20998  67.09964   8.46791 -15.85279 -16.81409 -12.48207   \n",
       "\n",
       "       var8      var9  ...     var81      var82     var83     var84     var85  \\\n",
       "0 -12.23257   7.83089  ...  13.01620  -54.40548  58.99367  15.37344   1.11144   \n",
       "1  -0.92019  18.76548  ...   5.66812  -19.68073  33.04964  42.87836  -9.90378   \n",
       "2  -2.35035  16.07017  ...   3.03800   26.05866 -50.92779  10.93792  -0.07568   \n",
       "3 -10.34124   3.55005  ...  34.57337 -171.70734 -16.96705 -46.67617 -12.51516   \n",
       "4  -9.37636  12.63699  ...   9.92661  -55.95724  64.92712 -17.72522  -1.49237   \n",
       "\n",
       "      var86      var87     var88      var89     var90  \n",
       "0 -23.08793   68.40795  -1.82223  -27.46348   2.26327  \n",
       "1 -32.22788   70.49388  12.04941   58.43453  26.92061  \n",
       "2  43.20130 -115.00698  -0.05859   39.67068  -0.66345  \n",
       "3  82.58061  -72.08993   9.90558  199.62971  18.85382  \n",
       "4  -7.50035   51.76631   7.88713   55.66926  28.74903  \n",
       "\n",
       "[5 rows x 91 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a) -- 2 pts\n",
    "\n",
    "The data set description text asks us to respect the below train/test split to\n",
    "avoid the \"producer effect\". That is, we want to make sure that no song from a single artist\n",
    "ends up in both the training and test set.\n",
    "\n",
    "Explain why it would be problematic to have\n",
    "some songs from an artist in the training set, and other songs from the same artist in the\n",
    "test set. (Hint: Remember that we want our test accuracy to predict how well the model\n",
    "will perform in practice on a song it hasn't learned about.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nYou won't be able to properly confirm that your algorithm will be able to generalize\\nto other real world data as the data you are training and data you are testing are similar.\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df[:463715]\n",
    "df_test = df[463715:]\n",
    "\n",
    "# conver to numpy\n",
    "# t_label is the year or target\n",
    "# x_labels are all the features\n",
    "\n",
    "# training features and data \n",
    "train_xs = df_train[x_labels].to_numpy() # (463715, 90) matrix\n",
    "# training targets \n",
    "train_ts = df_train[t_label].to_numpy()\n",
    "\n",
    "# testing features and data \n",
    "test_xs = df_test[x_labels].to_numpy()\n",
    "# testing targets\n",
    "test_ts = df_test[t_label].to_numpy()\n",
    "\n",
    "# Write your explanation here\n",
    "'''\n",
    "You won't be able to properly confirm that your algorithm will be able to generalize\n",
    "to other real world data as the data you are training and data you are testing are similar.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) -- 1 pts\n",
    "\n",
    "It can be beneficial to **normalize** the columns, so that each column (feature)\n",
    "has the *same* mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_means = df_train.mean()[1:].to_numpy() # the [1:] removes the mean of the \"year\" field\n",
    "feature_stds  = df_train.std()[1:].to_numpy()\n",
    "\n",
    "train_norm_xs = (train_xs - feature_means) / feature_stds\n",
    "test_norm_xs = (test_xs - feature_means) / feature_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_means average of all feature means over the training set\n",
    "# feature_means.shape => (90,) => a 90 x 1 vector\n",
    "# so in every row, it contains the average for that feature over the data in the training set\n",
    "\n",
    "# feature_stds is the standard deviation of every feature over the training set\n",
    "# standard deviation is root(variance), where variance is diff b/w expected value and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how in our code, we normalized the test set using the *training data means and standard deviations*.\n",
    "This is *not* a bug.\n",
    "\n",
    "Explain why it would be improper to compute and use test set means\n",
    "and standard deviations. (Hint: Remember what we want to use the test accuracy to measure.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your explanation here\n",
    "'''\n",
    "The reason why we normalize the test set using the training data means and standard deviations is \n",
    "because we want the test set to be like unseen data. So normalizing it to training data,\n",
    "our model will tell us how well it will preform on general unseen data.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c) -- 1 pts\n",
    "\n",
    "Finally, we'll move some of the data in our training set into a validation set.\n",
    "\n",
    "Explain why we should limit how many times we use the test set, and that we should use the validation\n",
    "set during the model building process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWe should only use the validation set to find optimal batch-sizes or learning rates, \\nit should not be used to directly train the data, but just tell us how our\\nmodel preforms.\\n\\nThe test set acts as completely new unseen data, that we apply our model to\\nafter validating it on the validation set and finding optimal lerning rates\\nand batch sizes. So the test set should only be used in the final testing moments.\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffle the training set\n",
    "reindex = np.random.permutation(len(train_xs))\n",
    "train_xs = train_xs[reindex]\n",
    "train_norm_xs = train_norm_xs[reindex]\n",
    "train_ts = train_ts[reindex]\n",
    "\n",
    "# validation set from here: https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7\n",
    "\n",
    "# use the first 50000 elements of `train_xs` as the validation set\n",
    "train_xs, val_xs           = train_xs[50000:], train_xs[:50000]\n",
    "train_norm_xs, val_norm_xs = train_norm_xs[50000:], train_norm_xs[:50000]\n",
    "train_ts, val_ts           = train_ts[50000:], train_ts[:50000]\n",
    "\n",
    "# Write your explanation here\n",
    "'''\n",
    "We should only use the validation set to find optimal batch-sizes or learning rates, \n",
    "it should not be used to directly train the data, but just tell us how our\n",
    "model preforms.\n",
    "\n",
    "The test set acts as completely new unseen data, that we apply our model to\n",
    "after validating it on the validation set and finding optimal lerning rates\n",
    "and batch sizes. So the test set should only be used in the final testing moments.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Classification\n",
    "\n",
    "We will first build a *classification* model to perform decade classification.\n",
    "These helper functions are written for you. All other code that you write in this\n",
    "section should be vectorized whenever possible, and you will be penalized for \n",
    "not vectorizing your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "def cross_entropy(t, y):\n",
    "    return -t * np.log(y) - (1 - t) * np.log(1 - y)\n",
    "\n",
    "def cost(y, t):\n",
    "    return np.mean(cross_entropy(t, y))\n",
    "\n",
    "def get_accuracy(y, t):\n",
    "    acc = 0\n",
    "    N = 0\n",
    "    for i in range(len(y)):\n",
    "        N += 1\n",
    "        if (y[i] >= 0.5 and t[i] == 1) or (y[i] < 0.5 and t[i] == 0):\n",
    "            acc += 1\n",
    "    return acc / N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a) -- 2 pts\n",
    "\n",
    "Write a function `pred` that computes the prediction `y` based on weights `w` and bias `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(w, b, X):\n",
    "    \"\"\"\n",
    "    Returns the prediction `y` of the target based on the weights `w` and scalar bias `b`.\n",
    "    Preconditions: np.shape(w) == (90,)\n",
    "             type(b) == float\n",
    "             np.shape(X) = (N, 90) for some N\n",
    "    \n",
    "    >>> pred(np.zeros(90), 1, np.ones([2, 90]))\n",
    "    array([0.73105858, 0.73105858]) # It's okay if your output differs in the last decimals\n",
    "    \"\"\"\n",
    "    N = len(X)\n",
    "    #print(N)\n",
    "    y = sigmoid(np.dot(X,w) + b*(np.ones(N)))\n",
    "    \n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) -- 3 pts\n",
    "\n",
    "Write a function `derivative_cost` that computes and returns the gradients \n",
    "$\\frac{\\partial\\mathcal{E}}{\\partial {\\bf w}}$ and\n",
    "$\\frac{\\partial\\mathcal{E}}{\\partial b}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_cost(X, y, t):\n",
    "    \"\"\"\n",
    "    Returns a tuple containing the gradients dEdw and dEdb.\n",
    "    \n",
    "    Precondition: np.shape(X) == (N, 90) for some N\n",
    "                np.shape(y) == (N,)\n",
    "                np.shape(t) == (N,)\n",
    "                \n",
    "    Postcondition: np.shape(dEdw) = (90,)\n",
    "           type(dEdb) = float\n",
    "    \"\"\"\n",
    "    # Your code goes here\n",
    "    # return np.zeros(90), 0.\n",
    "    \n",
    "    # dE/dW = dL/dy * dy/dz * dz/dw\n",
    "    # = (((y-t)*y^T)(1-y))^T * X\n",
    "    # = (y - t) * X\n",
    "    N = len(t)\n",
    "    dLdy = -t/y + (1-t)/(1-y)\n",
    "    \n",
    "    dEdw = 1/N * np.dot(X.T, y - t)\n",
    "    dEdb = 1/N * np.sum(y-t)\n",
    "    return (dEdw, dEdb)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c) -- 2 pts\n",
    "\n",
    "We can check that our derivative is implemented correctly using the finite difference rule. In 1D, the\n",
    "finite difference rule tells us that for small $h$, we should have\n",
    "\n",
    "$$\\frac{f(x+h) - f(x)}{h} \\approx f'(x)$$\n",
    "\n",
    "Prove to yourself (and your TA) that $\\frac{\\partial\\mathcal{E}}{\\partial b}$  is implement correctly\n",
    "by comparing the result from `derivative_cost` with the value of `(pred(w, b + h, X) - pred(w, b, X)) / h`.\n",
    "Justify your choice of `w`, `b`, and `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "derivative of b using cost and small h:  0.9955547533024856\n",
      "derivative of w using our derivative function:  0.9955547269470035\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nWe don't want to use too large of a 'b', 'X' and 'w'  because then the sigmoid function will \\nturn that into a 1, \\nAnd we don't want it to be too small or the sigmoid function will turn that\\ninto a 0.\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code goes here\n",
    "\n",
    "#∂E/∂b = dL/dy * dy/dw\n",
    "#      = dL/dy*  (pred(w, b + h, X) - pred(w, b, X)) / h\n",
    "#      = (-t/y + (1-t)(1-y)) * (pred(w, b + h, X) - pred(w, b, X)) / h\n",
    "\n",
    "X = .12221*np.ones([3,90])\n",
    "b = 0\n",
    "w = .492*np.ones(90)\n",
    "h = 0.000001\n",
    "t = np.zeros(3)\n",
    "dydb = (cost(pred(w, b + h, X), np.zeros(3)) - cost(pred(w, b, X), np.zeros(3))) / h\n",
    "print(\"derivative of b using cost and small h: \", dydb)\n",
    "dw, db = derivative_cost(X, pred(w, b, X), t)\n",
    "print(\"derivative of w using our derivative function: \", db)\n",
    "'''\n",
    "We don't want to use too large of a 'b', 'X' and 'w'  because then the sigmoid function will \n",
    "turn that into a 1, \n",
    "And we don't want it to be too small or the sigmoid function will turn that\n",
    "into a 0.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (d) -- 2 pts\n",
    "\n",
    "Prove to yourself (and your TA) that $\\frac{\\partial\\mathcal{E}}{\\partial {\\bf w}}$  is implement correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dEdw:  [0.00450506 0.00450506 0.00450506 0.00450506 0.00450506 0.00450506\n",
      " 0.00450506 0.00450506 0.00450506 0.00450506 0.00450506 0.00450506\n",
      " 0.00450506 0.00450506 0.00450506 0.00450506 0.00450506 0.00450506\n",
      " 0.00450506 0.00450506 0.00450506 0.00450506 0.00450506 0.00450506\n",
      " 0.00450506 0.00450506 0.00450506 0.00450506 0.00450506 0.00450506\n",
      " 0.00450506 0.00450506 0.00450506 0.00450506 0.00450506 0.00450506\n",
      " 0.00450506 0.00450506 0.00450506 0.00450506 0.00450506 0.00450506\n",
      " 0.00450506 0.00450506 0.00450506 0.00450506 0.00450506 0.00450506\n",
      " 0.00450506 0.00450506 0.00450506 0.00450506 0.00450506 0.00450506\n",
      " 0.00450506 0.00450506 0.00450506 0.00450506 0.00450506 0.00450506\n",
      " 0.00450506 0.00450506 0.00450506 0.00450506 0.00450506 0.00450506\n",
      " 0.00450506 0.00450506 0.00450506 0.00450506 0.00450506 0.00450506\n",
      " 0.00450506 0.00450506 0.00450506 0.00450506 0.00450506 0.00450506\n",
      " 0.00450506 0.00450506 0.00450506 0.00450506 0.00450506 0.00450506\n",
      " 0.00450506 0.00450506 0.00450506 0.00450506 0.00450506 0.00450506]\n",
      "------------------------\n",
      "dw from func:  [5.005625e-05 5.005625e-05 5.005625e-05 5.005625e-05 5.005625e-05\n",
      " 5.005625e-05 5.005625e-05 5.005625e-05 5.005625e-05 5.005625e-05\n",
      " 5.005625e-05 5.005625e-05 5.005625e-05 5.005625e-05 5.005625e-05\n",
      " 5.005625e-05 5.005625e-05 5.005625e-05 5.005625e-05 5.005625e-05\n",
      " 5.005625e-05 5.005625e-05 5.005625e-05 5.005625e-05 5.005625e-05\n",
      " 5.005625e-05 5.005625e-05 5.005625e-05 5.005625e-05 5.005625e-05\n",
      " 5.005625e-05 5.005625e-05 5.005625e-05 5.005625e-05 5.005625e-05\n",
      " 5.005625e-05 5.005625e-05 5.005625e-05 5.005625e-05 5.005625e-05\n",
      " 5.005625e-05 5.005625e-05 5.005625e-05 5.005625e-05 5.005625e-05\n",
      " 5.005625e-05 5.005625e-05 5.005625e-05 5.005625e-05 5.005625e-05\n",
      " 5.005625e-05 5.005625e-05 5.005625e-05 5.005625e-05 5.005625e-05\n",
      " 5.005625e-05 5.005625e-05 5.005625e-05 5.005625e-05 5.005625e-05\n",
      " 5.005625e-05 5.005625e-05 5.005625e-05 5.005625e-05 5.005625e-05\n",
      " 5.005625e-05 5.005625e-05 5.005625e-05 5.005625e-05 5.005625e-05\n",
      " 5.005625e-05 5.005625e-05 5.005625e-05 5.005625e-05 5.005625e-05\n",
      " 5.005625e-05 5.005625e-05 5.005625e-05 5.005625e-05 5.005625e-05\n",
      " 5.005625e-05 5.005625e-05 5.005625e-05 5.005625e-05 5.005625e-05\n",
      " 5.005625e-05 5.005625e-05 5.005625e-05 5.005625e-05 5.005625e-05]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.004455006295258481"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code goes here. You might find this below code helpful: but it's\n",
    "# up to you to figure out how/why, and how to modify the code\n",
    "# h = 0.000001\n",
    "# H = np.zeros(90)\n",
    "# H[0] = h\n",
    "\n",
    "X = .0001*np.ones([3,90])\n",
    "b = 0\n",
    "w = .25*np.ones(90)\n",
    "h = 0.000001*np.ones(90)\n",
    "\n",
    "t = np.zeros(3)\n",
    "\n",
    "dEdw = (cost(pred(w+h, b, X), t) - cost(pred(w, b, X), t)) / h\n",
    "dw, db = derivative_cost(X, pred(w, b, X), t)\n",
    "\n",
    "y = pred(w, b, X)\n",
    "# dLdy = (-t/y + (1-t)/(1-y))\n",
    "# dww = np.dot(dLdy, dydw)/3\n",
    "print(\"dEdw: \", dEdw)\n",
    "print(\"------------------------\")\n",
    "print(\"dw from func: \", dw)\n",
    "\n",
    "dEdw[0] - dw[0]\n",
    "'''\n",
    "Since the difference between the two is very minimal\n",
    "we can confirm our derivation is correct.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (e) -- 4 pts\n",
    "\n",
    "Now that you have a gradient function that works, we can actually run gradient descent! Complete\n",
    "the following code that will run stochastic: gradient descent training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gradient_descent(w0, b0, alpha=0.1, batch_size=100, max_iters=100):\n",
    "    \"\"\"Return the values of (w, b) after running gradient descent for max_iters.\n",
    "    We use:\n",
    "      - train_norm_xs and train_ts as the training set\n",
    "      - val_norm_xs and val_ts as the test set\n",
    "      - alpha as the learning rate\n",
    "      - (w0, b0) as the initial values of (w, b)\n",
    "  \n",
    "    Precondition: np.shape(w0) == (90,)\n",
    "                  type(b0) == float\n",
    "   \n",
    "    Postcondition: np.shape(w) == (90,)\n",
    "                   type(b) == float\n",
    "    \"\"\"\n",
    "    global train_xs\n",
    "    global train_norm_xs\n",
    "    global train_ts\n",
    "    \n",
    "    w = w0\n",
    "    b = b0\n",
    "    iter = 0\n",
    "    \n",
    "    while iter < max_iters:\n",
    "        # shuffle the training set (there is code above for how to do this)\n",
    "        \n",
    "        reindex = np.random.permutation(len(train_xs))\n",
    "        train_xs = train_xs[reindex]\n",
    "        train_norm_xs = train_norm_xs[reindex]\n",
    "        train_ts = train_ts[reindex]\n",
    "        \n",
    "        train_xs, val_xs           = train_xs[50000:], train_xs[:50000]\n",
    "        train_norm_xs, val_norm_xs = train_norm_xs[50000:], train_norm_xs[:50000]\n",
    "        train_ts, val_ts           = train_ts[50000:], train_ts[:50000]\n",
    "        \n",
    "        for i in range(0, len(train_norm_xs), batch_size): # iterate over each minibatch\n",
    "            # minibatch that we are working with:\n",
    "            X = train_norm_xs[i:(i + batch_size)]\n",
    "            t = train_ts[i:(i + batch_size), 0]\n",
    "      \n",
    "            # since len(train_norm_xs) does not divide batch_size evenly, we will skip over\n",
    "            # the \"last\" minibatch\n",
    "            if np.shape(X)[0] != batch_size:\n",
    "                continue\n",
    "      \n",
    "            # compute the prediction\n",
    "            y = pred(w,b,X)\n",
    "            \n",
    "            # update w and b\n",
    "            dw, db = derivative_cost(X,y,t)\n",
    "            # w ← w − α * dEdw\n",
    "            w = w - alpha*dw\n",
    "            # b ← b − α * dEdb\n",
    "            b = b - alpha*db\n",
    "            \n",
    "            #print(dw)\n",
    "            # increment the iteration count\n",
    "            iter += 1\n",
    "            \n",
    "            # compute and plot the *validation* loss and accuracy\n",
    "            if (iter % 10 == 0):\n",
    "                # using cost function?\n",
    "                train_cost = cost(y,t)\n",
    "                X_val = val_norm_xs[i:(i + batch_size)]\n",
    "                T_val = val_ts[i:(i + batch_size), 0]\n",
    "                if np.shape(X_val)[0] != batch_size:\n",
    "                    continue\n",
    "                val_y = pred(w,b,X_val)\n",
    "                val_cost = cost(val_y, T_val)\n",
    "                val_acc = get_accuracy(val_y, T_val)\n",
    "                print(\"Iter %d. [Val Acc %.0f%%, Loss %f] [Train Loss %f]\" % (\n",
    "                    iter, val_acc * 100, val_cost, train_cost))\n",
    "      \n",
    "            if iter >= max_iters:\n",
    "                break\n",
    "            #print(iter)\n",
    "    #print(b)\n",
    "    return w,b\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (f) -- 2 pts\n",
    "\n",
    "Call `run_gradient_descent` with the weights and biases all initialized to zero.\n",
    "Show that if `alpha` is too small, then convergence is slow.\n",
    "Also, show that if `alpha` is too large, then we do not converge at all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10. [Val Acc 68%, Loss 0.693141] [Train Loss 0.693143]\n",
      "Iter 20. [Val Acc 68%, Loss 0.693127] [Train Loss 0.693128]\n",
      "Iter 30. [Val Acc 60%, Loss 0.693135] [Train Loss 0.693137]\n",
      "Iter 40. [Val Acc 65%, Loss 0.693128] [Train Loss 0.693107]\n",
      "Iter 50. [Val Acc 60%, Loss 0.693119] [Train Loss 0.693105]\n",
      "Iter 60. [Val Acc 68%, Loss 0.693106] [Train Loss 0.693082]\n",
      "Iter 70. [Val Acc 68%, Loss 0.693091] [Train Loss 0.693103]\n",
      "Iter 80. [Val Acc 60%, Loss 0.693108] [Train Loss 0.693097]\n",
      "Iter 90. [Val Acc 71%, Loss 0.693063] [Train Loss 0.693078]\n",
      "Iter 100. [Val Acc 63%, Loss 0.693089] [Train Loss 0.693046]\n",
      "time for small alpha:  0.6561710834503174\n",
      "Iter 10. [Val Acc 64%, Loss 0.731518] [Train Loss 0.723095]\n",
      "Iter 20. [Val Acc 66%, Loss 0.688603] [Train Loss 0.698415]\n",
      "Iter 30. [Val Acc 72%, Loss 0.560412] [Train Loss 0.645157]\n",
      "Iter 40. [Val Acc 66%, Loss 0.562702] [Train Loss 0.641299]\n",
      "Iter 50. [Val Acc 67%, Loss 0.690069] [Train Loss 0.667883]\n",
      "Iter 60. [Val Acc 80%, Loss 0.550659] [Train Loss 0.770661]\n",
      "Iter 70. [Val Acc 70%, Loss 0.643948] [Train Loss 0.613910]\n",
      "Iter 80. [Val Acc 72%, Loss 0.613354] [Train Loss 0.589485]\n",
      "Iter 90. [Val Acc 70%, Loss 0.743510] [Train Loss 0.627942]\n",
      "Iter 100. [Val Acc 68%, Loss 0.639349] [Train Loss 0.611400]\n",
      "time for normal alpha:  0.5021071434020996\n"
     ]
    }
   ],
   "source": [
    "w0 = np.zeros(90)\n",
    "b0 = 0.\n",
    "# Write your code here\n",
    "import time\n",
    "start = time.time()\n",
    "run_gradient_descent(w0, b0, 0.00001)\n",
    "end = time.time()\n",
    "print(\"time for small alpha: \", end - start)\n",
    "start = time.time()\n",
    "run_gradient_descent(w0, b0, 1)\n",
    "end = time.time()\n",
    "print(\"time for normal alpha: \", end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10. [Val Acc 65%, Loss nan] [Train Loss nan]\n",
      "Iter 20. [Val Acc 61%, Loss nan] [Train Loss nan]\n",
      "Iter 30. [Val Acc 67%, Loss nan] [Train Loss nan]\n",
      "Iter 40. [Val Acc 65%, Loss nan] [Train Loss nan]\n",
      "Iter 50. [Val Acc 47%, Loss nan] [Train Loss nan]\n",
      "Iter 60. [Val Acc 47%, Loss nan] [Train Loss nan]\n",
      "Iter 70. [Val Acc 63%, Loss nan] [Train Loss nan]\n",
      "Iter 80. [Val Acc 65%, Loss nan] [Train Loss nan]\n",
      "Iter 90. [Val Acc 61%, Loss nan] [Train Loss nan]\n",
      "Iter 100. [Val Acc 69%, Loss nan] [Train Loss nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: RuntimeWarning: divide by zero encountered in true_divide\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in log\n",
      "  \"\"\"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in multiply\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nSo a large alpha (5) does not converge\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_gradient_descent(w0, b0, 100)\n",
    "'''\n",
    "So a large alpha (100) does not converge\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (g) -- 2 pts\n",
    "\n",
    "Find the optimial value of ${\\bf w}$ and $b$ using your code. Explain how you chose\n",
    "the learning rate $\\alpha$ and the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10. [Val Acc 65%, Loss 0.693104] [Train Loss 0.693118]\n",
      "Iter 20. [Val Acc 68%, Loss 0.692971] [Train Loss 0.693062]\n",
      "Iter 30. [Val Acc 74%, Loss 0.692911] [Train Loss 0.692934]\n",
      "Iter 40. [Val Acc 67%, Loss 0.692815] [Train Loss 0.692948]\n",
      "Iter 50. [Val Acc 57%, Loss 0.692926] [Train Loss 0.692788]\n",
      "Iter 60. [Val Acc 65%, Loss 0.692735] [Train Loss 0.692785]\n",
      "Iter 70. [Val Acc 61%, Loss 0.692776] [Train Loss 0.692874]\n",
      "Iter 80. [Val Acc 70%, Loss 0.692417] [Train Loss 0.692558]\n",
      "Iter 90. [Val Acc 64%, Loss 0.692525] [Train Loss 0.692533]\n",
      "Iter 100. [Val Acc 70%, Loss 0.692666] [Train Loss 0.692639]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nWe don't want to pick a \\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w0 = np.zeros(90)\n",
    "b0 = 0.\n",
    "# Write your code here\n",
    "# alpha=0.01, batch_size=150): gives 75% val accuracy\n",
    "w,b = run_gradient_descent(w0, b0, 0.0001, 100)\n",
    "'''\n",
    "We picked a small alpha, so even though convergence is slow it more accurately approaches\n",
    "the minimum.\n",
    "\n",
    "We didn't want to pick to high of a batch size as it will take too long,\n",
    "and too small of a batch size will lead to lower accuracy due to \n",
    "meaningless data effecting our model.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (h) -- 4 pts\n",
    "\n",
    "Using the values of `w` and `b` from part (g), compute your training accuracy, validation accuracy,\n",
    "and test accuracy. Are there any differences between those three values? If so, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy:  0.6589774961164648\n",
      "validation accuracy:  0.6563\n",
      "test accuracy:  0.6570211117567306\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "\n",
    "\n",
    "X = train_norm_xs\n",
    "y = pred(w,b,X)\n",
    "print(\"training accuracy: \", get_accuracy(y, train_ts))\n",
    "X = val_norm_xs\n",
    "y = pred(w,b,X)\n",
    "print(\"validation accuracy: \", get_accuracy(y, val_ts))\n",
    "\n",
    "X = test_norm_xs\n",
    "y = pred(w,b,X)\n",
    "print(\"test accuracy: \", get_accuracy(y, test_ts))\n",
    "\n",
    "'''\n",
    "The training accuracy should be the highest, followed by the validation accuracy\n",
    "then the test accuracy. This is because we train the model on the training data\n",
    "and adjust the learning rate and batch size according to the validation set.\n",
    "And the test accuracy has the lowest as it's new unseen data.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (i) -- 4 pts\n",
    "\n",
    "Writing a classifier like this is instructive, and helps you understand what happens when\n",
    "we train a model. However, in practice, we rarely write model building and training code\n",
    "from scratch. Instead, we typically use one of the well-tested libraries available in a package.\n",
    "\n",
    "Use `sklearn.linear_model.LogisticRegression` to build a linear classifier, and make predictions about the test set. Start by reading the\n",
    "[API documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "\n",
    "Compute the training, validation and test accuracy of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.linear_model\n",
    "model = sklearn.linear_model.LogisticRegression()\n",
    "model.fit(train_norm_xs, train_ts[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sklearnWeights = model.coef_[0]\n",
    "sklearnBias = model.intercept_[0]\n",
    "\n",
    "\n",
    "# val_y = pred(sklearnWeights, sklearnBias, val_norm_xs)\n",
    "# val_cost = cost(val_y, val_ts[:,0])\n",
    "# val_acc = get_accuracy(val_y, val_ts[:,0])\n",
    "\n",
    "training_accuracy = model.score(train_norm_xs, train_ts[:,0])\n",
    "vallidation_sklearn_accuracy = model.score(val_norm_xs, val_ts[:,0])\n",
    "test_accuracy = model.score(test_norm_xs, test_ts[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy:  0.7333269180539709\n",
      "validation accuracy:  0.73418\n",
      "test accuracy:  0.726980437730002\n"
     ]
    }
   ],
   "source": [
    "# print(\"validation cost: \",val_cost, \"validation accuracy: \", val_acc)\n",
    "\n",
    "print(\"training accuracy: \", training_accuracy)\n",
    "print(\"validation accuracy: \", vallidation_sklearn_accuracy)\n",
    "print(\"test accuracy: \", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Nearest Neighbour\n",
    "\n",
    "We will compare the nearest neighbour model with the model we built in the earlier parts.\n",
    "\n",
    "To make predictions for a new data point using k-nearest neighbour, we will need to:\n",
    "\n",
    "1. Compute the distance from this new data point to every element in the training set\n",
    "2. Select the top $k$ closest neighbour in the training set\n",
    "3. Find the most common label among those neighbours\n",
    "\n",
    "We'll use the validation test to select $k$. That is, we'll select the $k$ that gives the highest\n",
    "validation accuracy.\n",
    "\n",
    "Since we have a fairly large data set, computing the distance between a point in the validation\n",
    "set and all points in the training set will require more RAM than Google Colab provides.\n",
    "To make the comptuations tractable, we will:\n",
    "\n",
    "1. Use only a subset of the training set (only the first 100,000 elements)\n",
    "2. Use only a subset of the validation set (only the first 1000 elements)\n",
    "3. We will use the **cosine similarity** rather than Euclidean distance. We will also pre-scale\n",
    "   each element in training set and the validation set to be a unit vector, so that computing\n",
    "   the cosine similarity is equivalent to computing the dot product. To see this, recall that \n",
    "   $$cos(\\theta) = \\frac{v \\cdot w}{||v|| ||w||}$$. But if both ||v|| and ||w|| are zero, then\n",
    "   only the dot product remains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll need to take the first 100000 element of `train_norm_xs`\n",
    "# and scale each of its rows to be unit length\n",
    "xs = train_norm_xs[:100000]\n",
    "# compute the norms:\n",
    "norms = np.linalg.norm(xs, axis=1)\n",
    "# divide the xs by the norms. Because of numpy's broadcasting rules, we need to\n",
    "# transpose the matrices a couple of times:\n",
    "#   https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\n",
    "xs = (xs.T / norms).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a) -- 1 pt\n",
    "\n",
    "Create a numpy matrix `val_xs` that contains the first 1000 elements of `val_norm_xs`, scaled\n",
    "so that each of its rows is unit length. Follow the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_xs = val_norm_xs[:1000]\n",
    "val_norms = np.linalg.norm(val_xs, axis=1)\n",
    "val_xs = (val_xs.T / val_norms).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) -- 1 pt\n",
    "\n",
    "Our goal now is to compute the validation accuracy for a choice of $k$. This will\n",
    "require computing the distance between each song in the training set and each\n",
    "song in the validation set.\n",
    "\n",
    "This is actually quite straightforward, and can be done using one matrix\n",
    "computation operation!\n",
    "\n",
    "Compute all the distances between elements of `xs` and those of `val_xs`\n",
    "using a single call to `np.dot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_distances = np.dot ...\n",
    "# for every song in xs, find the distance to every song in val_xs\n",
    "val_distances = np.dot(xs, val_xs.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c) -- 3 pt\n",
    "\n",
    "Now that we have the distance pairs, we can use the matrix `val_distances`\n",
    "to find the set of neighbours for each point in our validation set and \n",
    "\n",
    "Find the validation accuracy assuming that we use $k = 10$. You may\n",
    "use the below helper function if you want, and the `get_accuracy` helper\n",
    "from the last section.\n",
    "\n",
    "You might also find it helpful to do parts (c) and (d) together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_neighbours(i, k=10):\n",
    "    \"\"\"\n",
    "    Return the indices of the top k-element of `xs` that are closests to\n",
    "    element `i` of the validation set `val_xs`. \n",
    "    \"\"\"\n",
    "  \n",
    "    # sort the element of the training set by distance to the i-th\n",
    "    # element of val_xs\n",
    "    neighbours = sorted(enumerate(val_distances[:, i]),\n",
    "                        key=lambda r: r[1],\n",
    "                        reverse=True)\n",
    "    # obtain the top k closest index and return it\n",
    "    neighbour_indices = [index for (index, dist) in neighbours[:k]]\n",
    "    return neighbour_indices \n",
    "\n",
    "def get_train_ts(indices):\n",
    "    \"\"\"\n",
    "    Return the labels of the corresponding elements in the training set `xs`.\n",
    "    Note that `xs` is the first 100,000 elements of `train_xs`, so we can\n",
    "    simply index `train_ts`.\n",
    "    \"\"\"\n",
    "    return train_ts[indices]\n",
    "\n",
    "\n",
    "# Write your code here\n",
    "\n",
    "def get_validation_accuracy(k):\n",
    "    trained_predictions = np.array([])\n",
    "    # for every sample in validation set\n",
    "    for i in range(len(val_xs)):\n",
    "        sample = val_xs[i]\n",
    "        neighbours = get_nearest_neighbours(i,k)\n",
    "        count_1s = 0\n",
    "        count_0s = 0\n",
    "\n",
    "        for j in range(len(neighbours)):\n",
    "            target = get_train_ts(neighbours[j])\n",
    "            if target == 0:\n",
    "                count_0s += 1\n",
    "            elif target == 1:\n",
    "                count_1s += 1\n",
    "\n",
    "        if count_1s > count_0s:\n",
    "            # classify as 1, or \"above 2000's\"\n",
    "            trained_predictions = np.append(trained_predictions, 1)\n",
    "        else:\n",
    "            # classify as 0, or \"below or equal 2000's\"\n",
    "            trained_predictions = np.append(trained_predictions, 0)\n",
    "\n",
    "\n",
    "    validation_accuracy = get_accuracy(trained_predictions, val_ts[:1000, 0])\n",
    "    return validation_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.663"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for k = 10, set k = 10 in get_nearest_neighbours\n",
    "get_validation_accuracy(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (d) -- 2 pts\n",
    "\n",
    "Compute the validation accuracy for $k = 50, 100, and 1000$.\n",
    "Which $k$ provides the best results? In other words, which kNN model would you deploy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.686"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code and solution here\n",
    "# for k = 50\n",
    "get_validation_accuracy(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.676"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for k = 100\n",
    "get_validation_accuracy(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for k = 1000\n",
    "get_validation_accuracy(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWe will deploy the kNN model where k = 50, as it has the highest accuracy.\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "We will deploy the kNN model where k = 50, as it has the highest accuracy.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (e) -- 4 pt\n",
    "\n",
    "Compute the test accuracy for the $k$ that you chose in the previous part.\n",
    "Use only a sample of 1000 elements from the test set to keep the problem tractable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.686"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code and solution here\n",
    "test_xs = test_norm_xs[:1000]\n",
    "test_norms = np.linalg.norm(test_xs, axis=1)\n",
    "test_xs = (test_xs.T / test_norms).T\n",
    "\n",
    "test_distances = np.dot(xs, test_xs.T)\n",
    "\n",
    "def get_nearest_neighbours2(i, k=10):\n",
    "    \"\"\"\n",
    "    Return the indices of the top k-element of `xs` that are closests to\n",
    "    element `i` of the test set `test_xs`. \n",
    "    \"\"\"\n",
    "  \n",
    "    # sort the element of the training set by distance to the i-th\n",
    "    # element of test_xs\n",
    "    neighbours = sorted(enumerate(test_distances[:, i]),\n",
    "                        key=lambda r: r[1],\n",
    "                        reverse=True)\n",
    "    # obtain the top k closest index and return it\n",
    "    neighbour_indices = [index for (index, dist) in neighbours[:k]]\n",
    "    return neighbour_indices \n",
    "\n",
    "def get_test_accuracy(k):\n",
    "    trained_predictions = np.array([])\n",
    "    # for every sample in validation set\n",
    "    for i in range(len(test_xs)):\n",
    "        sample = test_xs[i]\n",
    "        neighbours = get_nearest_neighbours(i,k)\n",
    "        count_1s = 0\n",
    "        count_0s = 0\n",
    "\n",
    "        for j in range(len(neighbours)):\n",
    "            target = get_train_ts(neighbours[j])\n",
    "            if target == 0:\n",
    "                count_0s += 1\n",
    "            elif target == 1:\n",
    "                count_1s += 1\n",
    "\n",
    "        if count_1s > count_0s:\n",
    "            # classify as 1, or \"above 2000's\"\n",
    "            trained_predictions = np.append(trained_predictions, 1)\n",
    "        else:\n",
    "            # classify as 0, or \"below or equal 2000's\"\n",
    "            trained_predictions = np.append(trained_predictions, 0)\n",
    "\n",
    "\n",
    "    test_accuracy = get_accuracy(trained_predictions, val_ts[:1000, 0])\n",
    "    return test_accuracy\n",
    "\n",
    "get_test_accuracy(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
